{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import copy, math, os, pickle, time, sys, os, random, argparse, pandas as pd, numpy as np, scipy.stats as ss\n",
    "import pathlib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils import weight_norm\n",
    "#from mmd_grud_utils import *\n",
    "\n",
    "import multiprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single CUDA device, comment for directions on multiple GPU\n",
    "'''\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See CUDA versions, CUDA must be available for GPU acceleration\n",
    "'''\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single CUDA device, comment for directions on multiple GPU\n",
    "'''\n",
    "torch.cuda.get_device_name(0)\n",
    "# torch.cuda.get_device_name(1)\n",
    "# torch.cuda.get_device_name(2)\n",
    "# torch.cuda.get_device_name(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See github directions on getting data access from Physionet.org\n",
    "'''\n",
    "DATA_FILEPATH     = \"./data/all_hourly_data.h5\"\n",
    "#RAW_DATA_FILEPATH = './all_hourly_data.h5'\n",
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "RESULTS_DIR     = \"./results/\"\n",
    "PROCESSED_DATA_DIR = \"./processed_data/\"\n",
    "\n",
    "\n",
    "def set_primary_seeds(seed):\n",
    "    print(\"Setting primary seeds...\")\n",
    "    if not seed:\n",
    "        seed = 1\n",
    "        \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "#     np.random.RandomState(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # for multiGPUs.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_primary_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "'''\n",
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_full_lvl2 = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "#data_full_raw  = pd.read_hdf(RAW_DATA_FILEPATH, 'vitals_labs') \n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "\n",
    "“Simple Imputation” scheme outlined in Che et al.:\n",
    "\n",
    "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan\n",
    "Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing\n",
    "Values. Scientific Reports 8, 1 (2018).\n",
    "\n",
    "'''\n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing to define 3-day and 7-day length of stay outcome labels\n",
    "'''\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2, raw = [df[\n",
    "    (df.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (df.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "] for df in (data_full_lvl2, data_full_lvl2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START: LOAD GRU-D SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "GRU-D Source from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "'''\n",
    "\n",
    "import time\n",
    "def to_3D_tensor(df):\n",
    "    idx = pd.IndexSlice\n",
    "    return np.dstack((df.loc[idx[:,:,:,i], :].values for i in sorted(set(df.index.get_level_values('hours_in')))))\n",
    "def prepare_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_3D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    \n",
    "    return utils.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last = True)\n",
    "\n",
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        assert in_features > 1 and out_features > 1, \"Passing in nonsense sizes\"\n",
    "        \n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        use_gpu=False\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu: self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:       self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "        if bias: self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:    self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None: self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.filter_square_matrix.mul(self.weight),\n",
    "            self.bias\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'\n",
    "        \n",
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, cell_size, hidden_size, X_mean, batch_size = 0, output_last = False):\n",
    "        \"\"\"\n",
    "        With minor modifications from https://github.com/zhiyongc/GRU-D/\n",
    "        Recurrent Neural Networks for Multivariate Times Series with Missing Values\n",
    "        GRU-D: GRU exploit two representations of informative missingness patterns, i.e., masking and time interval.\n",
    "        cell_size is the size of cell_state.\n",
    "        \n",
    "        Implemented based on the paper: \n",
    "        @article{che2018recurrent,\n",
    "          title={Recurrent neural networks for multivariate time series with missing values},\n",
    "          author={Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},\n",
    "          journal={Scientific reports},\n",
    "          volume={8},\n",
    "          number={1},\n",
    "          pages={6085},\n",
    "          year={2018},\n",
    "          publisher={Nature Publishing Group}\n",
    "        }\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        use_gpu = False\n",
    "        if use_gpu:\n",
    "            self.identity = torch.eye(input_size).cuda()\n",
    "            self.zeros = Variable(torch.zeros(batch_size, input_size).cuda())\n",
    "            self.zeros_h = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean).cuda())\n",
    "        else:\n",
    "            self.identity = torch.eye(input_size)\n",
    "            self.zeros = Variable(torch.zeros(batch_size, input_size))\n",
    "            self.zeros_h = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean))\n",
    "        \n",
    "#         print(\"linear size: \", input_size + hidden_size + self.mask_size)\n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # Wz, Uz are part of the same network. the bias is bz\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # Wr, Ur are part of the same network. the bias is br\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size) # W, U are part of the same network. the bias is b\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, self.hidden_size) # this was wrong in available version. remember to raise the issue\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, 2)\n",
    "        self.bn= torch.nn.BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True)\n",
    "        self.drop=nn.Dropout(p=0.5, inplace=False)\n",
    "        \n",
    "    def step(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x: input tensor\n",
    "            x_last_obsv: input tensor with forward fill applied\n",
    "            x_mean: the mean of each feature\n",
    "            h: the hidden state of the network\n",
    "            mask: the mask of whether or not the current value is observed\n",
    "            delta: the tensor indicating the number of steps since the last time a feature was observed.\n",
    "            \n",
    "        Returns:\n",
    "            h: the updated hidden state of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size()[0]\n",
    "        dim_size = x.size()[1]\n",
    "        \n",
    "        gamma_x_l_delta = self.gamma_x_l(delta)\n",
    "#         print(\"GXL: \", gamma_x_l_delta)\n",
    "        delta_x = torch.exp(-torch.max(self.zeros, gamma_x_l_delta)) #exponentiated negative rectifier\n",
    "#         print(\"delta_x\",delta_x)\n",
    "        \n",
    "        \n",
    "        gamma_h_l_delta = self.gamma_h_l(delta)\n",
    "#         print(\"GHL: \", gamma_h_l_delta)\n",
    "        delta_h = torch.exp(-torch.max(self.zeros_h, gamma_h_l_delta)) #self.zeros became self.zeros_h to accomodate hidden size != input size\n",
    "#         print(\"delta_h\", delta_h)\n",
    "        \n",
    "        x_mean = x_mean.repeat(batch_size, 1)\n",
    "#         print(\"x_mean\", x_mean)\n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "#         print(\"x\", x)\n",
    "        h = delta_h * h\n",
    "#         print(\"h\", h)\n",
    "        \n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "#         print(\"combined\", combined)\n",
    "#         print(\"combined_shape: \", combined.size())\n",
    "        z_int = self.zl(combined)\n",
    "#         print(z_int)\n",
    "        z = torch.sigmoid(self.zl(combined)) #sigmoid(W_z*x_t + U_z*h_{t-1} + V_z*m_t + bz)\n",
    "        r = torch.sigmoid(self.rl(combined)) #sigmoid(W_r*x_t + U_r*h_{t-1} + V_r*m_t + br)\n",
    "        combined_new = torch.cat((x, r*h, mask), 1)\n",
    "        h_tilde = torch.tanh(self.hl(combined_new)) #tanh(W*x_t +U(r_t*h_{t-1}) + V*m_t) + b\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        return h\n",
    "    \n",
    "    \n",
    "#     def step(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        \n",
    "#         batch_size = x.shape[0]\n",
    "#         dim_size = x.shape[1]\n",
    "        \n",
    "#         delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))\n",
    "#         delta_h = torch.exp(-torch.max(self.zeros, self.gamma_h_l(delta)))\n",
    "        \n",
    "#         x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "#         h = delta_h * h\n",
    "        \n",
    "#         combined = torch.cat((x, h, mask), 1)\n",
    "#         z = F.sigmoid(self.zl(combined))\n",
    "#         r = F.sigmoid(self.rl(combined))\n",
    "#         combined_r = torch.cat((x, r * h, mask), 1)\n",
    "#         h_tilde = F.tanh(self.hl(combined_r))\n",
    "#         h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "#         return h\n",
    "    \n",
    "    def forward(self, X, X_last_obsv, Mask, Delta):\n",
    "        batch_size = X.size(0)\n",
    "#         type_size = input.size(1)\n",
    "        step_size = X.size(1) # num timepoints\n",
    "        spatial_size = X.size(2) # num features\n",
    "        \n",
    "        Hidden_State = self.initHidden(batch_size)\n",
    "#         X = torch.squeeze(input[:,0,:,:])\n",
    "#         X_last_obsv = torch.squeeze(input[:,1,:,:])\n",
    "#         Mask = torch.squeeze(input[:,2,:,:])\n",
    "#         Delta = torch.squeeze(input[:,3,:,:])\n",
    "        \n",
    "        i = 1\n",
    "        outputs = None\n",
    "        for i in range(step_size):\n",
    "#             print(i)\n",
    "#             print(Hidden_State)\n",
    "            Hidden_State = self.step(\n",
    "                torch.squeeze(X[:,i:i+1,:], 1),\n",
    "                torch.squeeze(X_last_obsv[:,i:i+1,:], 1),\n",
    "                torch.squeeze(self.X_mean[:,i:i+1,:], 1),\n",
    "                Hidden_State,\n",
    "                torch.squeeze(Mask[:,i:i+1,:], 1),\n",
    "                torch.squeeze(Delta[:,i:i+1,:], 1),\n",
    "            )\n",
    "            if outputs is None:\n",
    "                outputs = Hidden_State.unsqueeze(1)\n",
    "            else:\n",
    "                outputs = torch.cat((Hidden_State.unsqueeze(1), outputs), 1)\n",
    "#         time.sleep(10)\n",
    "#         print(\"outputs:\",outputs)\n",
    "#         test = self.fc(Hidden_State)\n",
    "#         print(\"test\",test)\n",
    "#         test2 = self.bn(test)\n",
    "#         print(\"test2\",test2)\n",
    "#         test3 = self.drop(test2)\n",
    "#         print(\"test3\",test3)\n",
    "        \n",
    "        # we want to predict a binary outcome\n",
    "        #Apply 50% dropout and batch norm here\n",
    "        return self.drop(self.bn(self.fc(Hidden_State)))\n",
    "                \n",
    "#         if self.output_last:\n",
    "#             return outputs[:,-1,:]\n",
    "#         else:\n",
    "#             return outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "#         use_gpu = torch.cuda.is_available()\n",
    "        use_gpu = False\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State\n",
    "\n",
    "        \n",
    "def Train_Model(\n",
    "    model, train_dataloader, valid_dataloader, num_epochs = 300, patience = 3, min_delta = 1e-5, learning_rate=1e-3, batch_size=None\n",
    "):\n",
    "    print('Model Structure: ', model)\n",
    "    print('Start Training ... epochs: ', num_epochs)\n",
    "    \n",
    "    model\n",
    "    \n",
    "    if (type(model) == nn.modules.container.Sequential):\n",
    "        output_last = model[-1].output_last\n",
    "        print('Output type dermined by the last layer')\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "        print('Output type dermined by the model')\n",
    "        \n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_nll=torch.nn.NLLLoss()\n",
    "    loss_CEL=torch.nn.CrossEntropyLoss()\n",
    "    loss_L1 = torch.nn.L1Loss()\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, alpha=0.99)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    use_gpu = False\n",
    "#     use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    interval = 100\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    losses_epochs_train = []\n",
    "    losses_epochs_valid = []\n",
    "    \n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "    \n",
    "    # Variables for Early Stopping\n",
    "    is_best_model = 0\n",
    "    patient_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        trained_number = 0\n",
    "        \n",
    "        valid_dataloader_iter = iter(valid_dataloader)\n",
    "        \n",
    "        losses_epoch_train = []\n",
    "        losses_epoch_valid = []\n",
    "        \n",
    "        for X, labels in train_dataloader:\n",
    "#             use_gpu = torch.cuda.is_available()\n",
    "            use_gpu=False\n",
    "            X = X.numpy()\n",
    "            mask        = torch.from_numpy(X[:, np.arange(0, X.shape[1], 3), :].astype(np.float32))\n",
    "            measurement = torch.from_numpy(X[:, np.arange(1, X.shape[1], 3), :].astype(np.float32))\n",
    "            time_       = torch.from_numpy(X[:, np.arange(2, X.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "            mask = torch.transpose(mask, 1, 2)\n",
    "            measurement = torch.transpose(measurement, 1, 2)\n",
    "            time_ = torch.transpose(time_, 1, 2)\n",
    "            measurement_last_obsv = measurement            \n",
    "\n",
    "            assert measurement.size()[0] == batch_size, \"Batch Size doesn't match! %s\" % str(measurement.size())\n",
    "\n",
    "            if use_gpu:\n",
    "                convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "                X, X_last_obsv, Mask, Delta, labels = map(convert_to_cuda, [measurement, measurement_last_obsv, mask, time_, labels])\n",
    "            else: \n",
    "                convert_to_tensor=lambda x: Variable(x)\n",
    "                X, X_last_obsv, Mask, Delta, labels  = map(convert_to_tensor, [measurement, measurement_last_obsv, mask, time_, labels])\n",
    "            \n",
    "            model.zero_grad()\n",
    "            prediction=model(X, X_last_obsv, Mask, Delta)\n",
    "    \n",
    "            if output_last:\n",
    "                loss_train = loss_CEL(torch.squeeze(prediction), torch.squeeze(labels))\n",
    "            else:\n",
    "                full_labels = torch.cat((inputs[:,1:,:], labels), dim = 1)\n",
    "                loss_train = loss_MSE(outputs, full_labels)\n",
    "        \n",
    "            losses_train.append(loss_train.data)\n",
    "            losses_epoch_train.append(loss_train.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_train.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "             # validation \n",
    "            try: \n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "                X_val = X_val.numpy()\n",
    "                mask_val        = torch.from_numpy(X_val[:, np.arange(0, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                measurement_val = torch.from_numpy(X_val[:, np.arange(1, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                time_val       = torch.from_numpy(X_val[:, np.arange(2, X_val.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "                mask_val = torch.transpose(mask_val, 1, 2)\n",
    "                measurement_val = torch.transpose(measurement_val, 1, 2)\n",
    "                time_val = torch.transpose(time_val, 1, 2)\n",
    "                measurement_last_obsv_val = measurement_val\n",
    "            except StopIteration:\n",
    "                valid_dataloader_iter = iter(valid_dataloader)\n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "                X_val = X_val.numpy()\n",
    "                mask_val        = torch.from_numpy(X_val[:, np.arange(0, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                measurement_val = torch.from_numpy(X_val[:, np.arange(1, X_val.shape[1], 3), :].astype(np.float32))\n",
    "                time_val       = torch.from_numpy(X_val[:, np.arange(2, X_val.shape[1], 3), :].astype(np.float32))\n",
    "            \n",
    "                mask_val = torch.transpose(mask_val, 1, 2)\n",
    "                measurement_val = torch.transpose(measurement_val, 1, 2)\n",
    "                time_val = torch.transpose(time_val, 1, 2)\n",
    "                measurement_last_obsv_val = measurement_val\n",
    "            \n",
    "            if use_gpu:\n",
    "                convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "                X_val, X_last_obsv_val, Mask_val, Delta_val, labels_val = map(convert_to_cuda, [measurement_val, measurement_last_obsv_val, mask_val, time_val, labels_val])\n",
    "            else: \n",
    "#                 inputs, labels = Variable(inputs), Variable(labels)\n",
    "                convert_to_tensor=lambda x: Variable(x)\n",
    "                X_val, X_last_obsv_val, Mask_val, Delta_val, labels_val = map(convert_to_tensor, [measurement_val, measurement_last_obsv_val, mask_val, time_val, labels_val])\n",
    "            \n",
    "                \n",
    "            model.zero_grad()\n",
    "            \n",
    "            prediction_val = model(X_val, X_last_obsv_val, Mask_val, Delta_val)\n",
    "\n",
    "            if output_last:\n",
    "                loss_valid =loss_CEL(torch.squeeze(prediction_val), torch.squeeze(labels_val))\n",
    "            else:\n",
    "                raise NotImplementedError(\"Should be output last!\")\n",
    "                full_labels_val = torch.cat((inputs_val[:,1:,:], labels_val), dim = 1)\n",
    "                loss_valid = loss_MSE(outputs_val, full_labels_val)\n",
    "\n",
    "            losses_valid.append(loss_valid.data)\n",
    "            losses_epoch_valid.append(loss_valid.data)\n",
    "            \n",
    "            # output\n",
    "            trained_number += 1\n",
    "            \n",
    "        avg_losses_epoch_train = sum(losses_epoch_train).cpu().numpy() / float(len(losses_epoch_train))\n",
    "        avg_losses_epoch_valid = sum(losses_epoch_valid).cpu().numpy() / float(len(losses_epoch_valid))\n",
    "        losses_epochs_train.append(avg_losses_epoch_train)\n",
    "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
    "        \n",
    "        \n",
    "        # Early Stopping\n",
    "        if epoch == 0:\n",
    "            is_best_model = 1\n",
    "            best_model = model\n",
    "            min_loss_epoch_valid = 10000.0\n",
    "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "        else:\n",
    "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
    "                is_best_model = 1\n",
    "                best_model = model\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid \n",
    "                patient_epoch = 0\n",
    "            else:\n",
    "                is_best_model = 0\n",
    "                patient_epoch += 1\n",
    "                if patient_epoch >= patience:\n",
    "                    print('Early Stopped at Epoch:', epoch)\n",
    "                    break\n",
    "        \n",
    "        # Print training parameters\n",
    "        cur_time = time.time()\n",
    "        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n",
    "                    epoch, \\\n",
    "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
    "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
    "                    np.around([cur_time - pre_time] , decimals=2),\\\n",
    "                    is_best_model) )\n",
    "        pre_time = cur_time\n",
    "#         if epoch==1:\n",
    "#             break\n",
    "                \n",
    "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]\n",
    "\n",
    "def predict_proba(model, dataloader):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        model: GRU-D model\n",
    "        test_dataloader: containing batches of measurement, measurement_last_obsv, mask, time_, labels\n",
    "    Returns:\n",
    "        predictions: size[num_samples, 2]\n",
    "        labels: size[num_samples]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    use_gpu = False\n",
    "#     use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    probabilities = []\n",
    "    labels        = []\n",
    "    ethnicities   = []\n",
    "    genders       = []\n",
    "    for X, label in dataloader:\n",
    "        X = X.numpy()\n",
    "        mask        = torch.from_numpy(X[:, np.arange(0, X.shape[1], 3), :].astype(np.float32))\n",
    "        measurement = torch.from_numpy(X[:, np.arange(1, X.shape[1], 3), :].astype(np.float32))\n",
    "        time_       = torch.from_numpy(X[:, np.arange(2, X.shape[1], 3), :].astype(np.float32))\n",
    "\n",
    "        mask = torch.transpose(mask, 1, 2)\n",
    "        measurement = torch.transpose(measurement, 1, 2)\n",
    "        time_ = torch.transpose(time_, 1, 2)\n",
    "        measurement_last_obsv = measurement            \n",
    "\n",
    "        if use_gpu:\n",
    "            convert_to_cuda=lambda x: Variable(x.cuda())\n",
    "            X, X_last_obsv, Mask, Delta, label = map(convert_to_cuda, [measurement, measurement_last_obsv, mask, time_, label])\n",
    "        else: \n",
    "#                 inputs, labels = Variable(inputs), Variable(labels)\n",
    "            convert_to_tensor=lambda x: Variable(x)\n",
    "            X, X_last_obsv, Mask, Delta, label  = map(convert_to_tensor, [measurement, measurement_last_obsv, mask, time_, label])\n",
    "\n",
    "        \n",
    "        prob = model(X, X_last_obsv, Mask, Delta)\n",
    "        probabilities.append(torch.squeeze(prob).detach().cpu().data.numpy())\n",
    "        labels.append(torch.squeeze(label).detach().cpu().data.numpy())\n",
    "\n",
    "    return probabilities, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subjects that were admited before and after 2180 [2100,2200]\n",
    "Ys['admittime'] = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['admittime']]\n",
    "split_date = np.datetime64('2180-01-01')\n",
    "Ys_int = Ys[Ys['admittime']<split_date]\n",
    "Ys_ext = Ys[Ys['admittime']>=split_date]\n",
    "subjects_int = Ys_int.index\n",
    "subjects_ext = Ys_ext.index\n",
    "del(Ys_int)\n",
    "del(Ys_ext)\n",
    "Ys.drop(labels='admittime',axis=1,inplace=True)\n",
    "Ys.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_subj_idx,  Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "# shuffle the dataset\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "\n",
    "# standardize the whole dataset\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2.loc[:, idx[:,'mean']].mean(axis=0), lvl2.loc[:, idx[:,'mean']].std(axis=0)\n",
    "lvl2.loc[:, idx[:,'mean']] = (lvl2.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "\n",
    "# impute missing values\n",
    "lvl2 = simple_imputer(lvl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split internal and sets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "X_int = lvl2.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "X_ext = lvl2.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "Y_int = Ys.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "Y_ext = Ys.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))\n",
    "\n",
    "all_int_subjects = list(\n",
    "    np.random.permutation(Y_int.index.get_level_values('subject_id').values)\n",
    ")\n",
    "\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some helper data structures to store and save predictions\n",
    "'''\n",
    "class Logger():\n",
    "    def __init__(self, optional_cols=None):\n",
    "        self.columns=['task_name','fold','prediction_no','index','y_true','y_score','censoring']\n",
    "        if (optional_cols is None):\n",
    "            self.df=pd.DataFrame(columns=self.columns)\n",
    "            self.has_optional_cols=False\n",
    "        else:\n",
    "            self.df=pd.DataFrame(columns=self.columns+optional_cols)\n",
    "            self.has_optional_cols=True\n",
    "            self.optional_cols=optional_cols\n",
    "        self._rocs=[]\n",
    "        self._prediction_no=0\n",
    "        return\n",
    "    \n",
    "    def append_logger(self,indices, y_true, y_score, label, censoring=None, optional_dict=None,fold=0):\n",
    "        y_true=np.array(y_true).astype(int)\n",
    "        y_score=np.array(y_score).astype(float)\n",
    "        \n",
    "\n",
    "        if ((y_true.shape[0]!=y_score.shape[0])):\n",
    "            raise ValueError(\"Shapes of input matrices must match\")\n",
    "        \n",
    "            \n",
    "        self._n=y_true.shape[0]\n",
    "\n",
    "        if(censoring is None):\n",
    "            cens = self._n*[math.nan]\n",
    "            censoring=np.array(censoring)\n",
    "        else:\n",
    "            cens=censoring\n",
    "        \n",
    "        arr=np.array([self._n*[label],\n",
    "                      self._n*[fold],\n",
    "                      self._n*[self._prediction_no],\n",
    "                      list(indices),\n",
    "                      list(y_true),\n",
    "                      list(y_score),\n",
    "                      list(cens)\n",
    "              ]).transpose()\n",
    "\n",
    "        to_append=pd.DataFrame(arr, columns=self.columns)\n",
    "        if(self.has_optional_cols):\n",
    "            \n",
    "            for column, value in optional_dict.items():\n",
    "                to_append.loc[:,column]=value\n",
    "\n",
    "        self.df=self.df.append(to_append)\n",
    "        self._prediction_no=self._prediction_no+1\n",
    "        \n",
    "def preds_df_to_int(df):\n",
    "    df_test = df\n",
    "    type_dict = {}\n",
    "    cast=['fold','prediction_no','y_true']\n",
    "    for col in cast:\n",
    "        type_dict[col] = 'int64'\n",
    "    df_test = df_test.astype(dtype=type_dict)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3D_tensor(df):\n",
    "    idx = pd.IndexSlice\n",
    "    return np.dstack((df.loc[idx[:,:,:,i], :].values for i in sorted(set(df.index.get_level_values('hours_in')))))\n",
    "\n",
    "def to_3D_tensor2(df):\n",
    "    idx = pd.IndexSlice\n",
    "    return torch.from_numpy(np.dstack((df.loc[idx[:,:,:,i], :].values for i in sorted(set(df.index.get_level_values('hours_in'))))))\n",
    "\n",
    "def to_2D_tensor(df):\n",
    "    # idx = pd.IndexSlice\n",
    "    dl = list()\n",
    "#     l1 = 0\n",
    "    for r_idx, row in df.iterrows():\n",
    "        d = df.loc[r_idx,'data']\n",
    "\n",
    "        dl.append(d)\n",
    "    return np.stack(tuple(dl))\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def prepare_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_3D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    return utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=shuffle, drop_last = True)\n",
    "\n",
    "def prepare_2d_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_2D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    return utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=shuffle, drop_last = True)\n",
    "\n",
    "\n",
    "def stack_dataframe(df):\n",
    "    df.columns = df.columns.map('_'.join)\n",
    "    df2 = pd.DataFrame(index=df.index.droplevel(level=3).drop_duplicates())\n",
    "    df2['data'] = ''\n",
    "    for idx, df in df.groupby(level=[0,1,2]):\n",
    "        data = []\n",
    "        cols = list(df)\n",
    "        n = len(df[cols[0]])\n",
    "        for i in range(n):\n",
    "            for col in cols:\n",
    "                k = len(df[col])\n",
    "                if n != k:\n",
    "                    print(\"different len: \", n, \"vs. \", k)\n",
    "                data.append(df[col].iloc[i])\n",
    "        df2.loc[idx,'data'] = data\n",
    "    return df2\n",
    "\n",
    "def stack_dataframe_alt(df):\n",
    "    df.columns = df.columns.map('_'.join)\n",
    "    df2 = pd.DataFrame(index=df.index.droplevel(level=3).drop_duplicates())\n",
    "    df2['data'] = ''\n",
    "    for idx, df in df.groupby(level=[0,1,2]):\n",
    "        data = []\n",
    "        cols = list(df)\n",
    "        n = len(df[cols[0]])\n",
    "        for col in cols:\n",
    "            tmp_array = df[col]\n",
    "            for val in tmp_array:\n",
    "                data.append(val)\n",
    "        df2.loc[idx,'data'] = data\n",
    "    return df2\n",
    "\n",
    "def ps_spawn(i, shared_dict, df):\n",
    "    res = stack_dataframe_alt(df)\n",
    "    shared_dict[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "early_stop_frac = 0.1\n",
    "hyperparams_fixed = {\n",
    "    'early_stop_frac': early_stop_frac,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95), \n",
    "    'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "    'num_epochs': ss.randint(15, 150),\n",
    "    #'num_epochs': ss.randint(1, 2),\n",
    "    'patience': ss.randint(3, 7),\n",
    "    'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "    'seed': ss.randint(1, 10000),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)\n",
    "RESULTS_PATH = './results/extraction_baselines_gru-d.pkl'\n",
    "#with open(RESULTS_PATH, mode='rb') as f: results = pickle.load(f)\n",
    "results = {}\n",
    "print(GRU_D_hyperparams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GRU_D_hyperparams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_int_alt, subjects_ext_alt = [df.index.get_level_values('subject_id') for df in (X_int, X_ext)]\n",
    "subjects_int_alt = set(subjects_int_alt)\n",
    "subjects_ext_alt = set(subjects_ext_alt)\n",
    "print(len(subjects_int_alt))\n",
    "print(len(subjects_ext_alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "hyperparams_list = GRU_D_hyperparams_list.copy()\n",
    "sss = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "outcomes = ['los_3', 'los_7', 'mort_icu', 'mort_hosp']\n",
    "preds_int = Logger()\n",
    "preds_ext = Logger()\n",
    "subjects_int_alt = list(subjects_int_alt)\n",
    "subjects_ext_alt = list(subjects_ext_alt)\n",
    "batch_size=16\n",
    "for t in outcomes:\n",
    "    print(\"Outcome:\", t)\n",
    "    int_save_str = RESULTS_DIR + '10Fold_GRU_int_' + str(t)\n",
    "    ext_save_str = RESULTS_DIR + '10Fold_GRU_ext_' + str(t)\n",
    "    fold=1\n",
    "    best_fold_F1, best_fold_rmse, best_fold_auc, best_fold_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "    best_fold_model_name = \"N/A\"\n",
    "    best_fold = -1\n",
    "\n",
    "    for train_subj_idx, test_subj_idx in sss.split(np.zeros(len(subjects_int_alt)), Y_int[t]):\n",
    "        train_subj = list(np.array(subjects_int_alt)[train_subj_idx])\n",
    "        test_subj  = list(np.array(subjects_int_alt)[test_subj_idx])\n",
    "        # Internal: 10-fold cross validation for training split\n",
    "        best_F1, best_rmse, best_auc, best_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "        best_hyperparams = None\n",
    "        early_stop_frac = hyperparams_fixed['early_stop_frac']\n",
    "        best_preds = []\n",
    "        print(\"Evaluating GRU for Outcome: %s, Fold: %d\"%(t,fold))\n",
    "\n",
    "        \n",
    "        set_primary_seeds(SEED)\n",
    "        all_train_subjects, N = np.random.permutation(list(train_subj)), len(train_subj)\n",
    "        N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "        train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "        early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "        print(\"Train subjects length: \", len(train_subjects))\n",
    "        print(\"ES/valid subjects length: \", len(early_stop_subjects))\n",
    "        \n",
    "        [(X_train_obs_tmp, X_train_early_stop_tmp, X_test_tmp), (Ys_train_obs, Ys_train_early_stop, Ys_test)] = [\n",
    "            [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subjects, early_stop_subjects,  test_subj)] \\\n",
    "            for df in (X_int, Y_int)\n",
    "        ]\n",
    "\n",
    "        if t in ['mort_icu', 'los_3', 'mort_hosp', 'los_7']:\n",
    "            output_last = True\n",
    "        elif t=='los_icu':\n",
    "            output_last = False\n",
    "        else:\n",
    "            print(\"invalid label for 'output_last' check\")\n",
    "        print(\"X_train_obs nan count: \", X_train_obs_tmp.isna().sum().sum())\n",
    "        X_mean = np.nanmean(\n",
    "            to_3D_tensor(\n",
    "                X_train_obs_tmp.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "                np.where((X_train_obs_tmp.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "            ),\n",
    "            axis=0, keepdims=True\n",
    "        ).transpose([0, 2, 1])\n",
    "        \n",
    "        X_mean = np.nan_to_num(X_mean, copy=True, nan=0, posinf=X_mean.max(), neginf=X_mean.min())\n",
    "\n",
    "        base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "        print(\"Total number of hyperparameters combos: \", len(hyperparams_list))\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            model_name = \"Fold%d_ParamSet%d\"%(fold,i)\n",
    "            print(\"Beginning Evaluation for: %s\"%(model_name))\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs_tmp, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(X_train_early_stop_tmp, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "            test_dataloader        = prepare_dataloader(X_test_tmp, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            print(\"init dataloaders complete\")\n",
    "            batch_size = 16\n",
    "            set_primary_seeds(SEED)\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model_hyperparams['batch_size'] = batch_size\n",
    "            model = GRUD(**model_hyperparams)\n",
    "            \n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('learning_rate', 'num_epochs', 'patience', 'early_stop_frac')}\n",
    "            )\n",
    "            #model = model.to(torch.device('cuda'))\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in model_hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "\n",
    "            \n",
    "            if test_dataloader is not None:\n",
    "                set_primary_seeds(SEED)\n",
    "                probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "                y_scores      = np.concatenate(probabilities_test)\n",
    "                y_true        = np.concatenate(labels_test)\n",
    "                subject_idx = list(range(0,len(y_scores)))\n",
    "                print(\"y_true:\", y_true.shape)\n",
    "                print(\"y_scores:\", y_scores.shape)\n",
    "                \n",
    "                print(\"Internal validation testing for our best model: %s, on target %s\" % (model_name, t))\n",
    "                if output_last:\n",
    "                    y_pred = []\n",
    "                    m = nn.Softmax(dim=1)\n",
    "                    y_scores_sm = m(torch.from_numpy(y_scores))\n",
    "                    y_pred = np.argmax(y_scores_sm, 1).numpy()\n",
    "                    y_score = y_scores_sm.numpy()[:,1]\n",
    "                    print(\"y_score:\",y_score.shape)\n",
    "\n",
    "                    auc = roc_auc_score(y_true, y_score)\n",
    "                    auprc = average_precision_score(y_true, y_score)\n",
    "                    acc   = accuracy_score(y_true, y_pred)\n",
    "                    prec = precision_score(y_true, y_pred)\n",
    "                    rec = recall_score(y_true, y_pred)\n",
    "                    F1    = f1_score(y_true, y_pred)\n",
    "                    print(\"auc->%f, auprc->%f, acc->%f, prec->%f, rec->%f, F1->%f\" % (auc, auprc, acc, prec, rec, F1))\n",
    "                    if auc > best_auc:\n",
    "                        best_auc, best_hyperparams = auc, model_hyperparams\n",
    "#                         print(\"New Best AUC within Fold (%d): %.2f @ hyperparams = %s\" % (fold, 100*best_auc, repr((best_hyperparams))))\n",
    "                        print(\"New Best AUC within Fold (%d): %.2f\" % (fold, 100*best_auc))\n",
    "                        # save our best model just in case we want it later\n",
    "#                         torch.save(best_model.module.state_dict(), 'results/best_model.pt')\n",
    "                        best_preds = y_score\n",
    "                else:\n",
    "                    mse = mean_squared_error(y_score, labels_test)\n",
    "                    rmse = sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_score, labels_test)\n",
    "                    pred_mean = np.mean(labels_test)\n",
    "                    pred_std = np.std(labels_test)\n",
    "                    label_mean = np.mean(y_score)\n",
    "                    label_std = np.std(y_score)\n",
    "                    print(\"mse->%f, rmse->%f, MAE->%f, (pred mean, pred_std)->(%f,%f), (label_mean, label_std)->(%f,%f)\" % (mse, rmse, mae, pred_mean, pred_std, label_mean, label_std))\n",
    "#                     results[model_name][t][n] = None, model_hyperparams, mse, rmse\n",
    "                    if rmse > best_rmse:\n",
    "                        best_rmse, best_hyperparams = rmse, model_hyperparams\n",
    "#                         print(\"New Best RMSE within Fold (%d): %.2f @ hyperparams = %s\" % (fold, best_rmse, repr((best_hyperparams))))\n",
    "                        print(\"New Best RMSE within Fold (%d): %.2f\" % (fold, best_rmse))\n",
    "                        # save our best model just in case we want it later\n",
    "#                         torch.save(best_model.module.state_dict(), 'results/best_model.pt')\n",
    "                        best_preds = y_score\n",
    "        if output_last:\n",
    "            if best_auc > best_fold_auc:\n",
    "                best_fold_auc, best_fold_hyperparams = best_auc, best_hyperparams\n",
    "                best_fold_model_name = model_name\n",
    "#                 print(\"New Best AUC across all folds: %.2f @ hyperparams = %s\" % (100*best_fold_auc, repr((best_fold_hyperparams))))\n",
    "                print(\"New Best AUC across all folds: %.2f\" % (100*best_fold_auc))\n",
    "                # save our best model just in case we want it later\n",
    "#                 torch.save(best_model.module.state_dict(), 'results/best_overall_model.pt')\n",
    "                best_fold = fold\n",
    "        else:\n",
    "            if best_rmse > best_fold_rmse:\n",
    "                best_fold_rmse, best_fold_hyperparams = best_rmse, best_hyperparams\n",
    "                best_fold_model_name = model_name\n",
    "                print(\"New Best RMSE across all folds: %.2f \"% (best_fold_rmse))\n",
    "#                 print(\"New Best RMSE across all folds: %.2f @ hyperparams = %s\" % (best_fold_rmse, repr((best_fold_hyperparams))))\n",
    "                # save our best model just in case we want it later\n",
    "    #                 torch.save(best_model.module.state_dict(), 'results/best_overall_model.pt')\n",
    "                best_fold = fold\n",
    "        assert len(y_true) == len(best_preds), \"Labels (%d) and preds lengths (%d) dont match\"%(len(y_true),len(best_preds))\n",
    "        print(\"Appending best predictions from Fold %d. Length: %d\"%(fold, len(best_preds)))\n",
    "        preds_int.append_logger(subject_idx, y_true, best_preds, label=t, fold=fold)\n",
    "        fold+=1\n",
    "        print(\"Best hyperparams set: %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams)))) \n",
    "        print()\n",
    "        \n",
    "    # External: train/validation and testing on all data\n",
    "    print(\"Begin external testing\")\n",
    "    print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "    print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "    print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "    print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))\n",
    "\n",
    "    train_subj = list(np.array(subjects_int_alt))\n",
    "    test_subj  = list(np.array(subjects_ext_alt))\n",
    "    print(\"External evaluation for GRU-D for Outcome: %s\"%(t))\n",
    "    set_primary_seeds(SEED)\n",
    "    all_train_subjects, N = np.random.permutation(list(train_subj)) ,len(train_subj)\n",
    "    all_test_subjects = np.random.permutation(list(test_subj))\n",
    "    N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "    early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "    print(\"Train subjects length: \", len(train_subjects))\n",
    "    print(\"ES/valid subjects length: \", len(early_stop_subjects))  \n",
    "    \n",
    "    [(X_train_obs_tmp, X_train_early_stop_tmp), (Ys_train_obs, Ys_train_early_stop)] = [\n",
    "        [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subjects, early_stop_subjects)] \\\n",
    "        for df in (X_int, Y_int)\n",
    "    ]\n",
    "\n",
    "    X_test_tmp = X_ext.copy()\n",
    "    Ys_test = Y_ext.copy()\n",
    "#     [(X_test_tmp), (Ys_test)] = [\n",
    "#         [df[df.index.get_level_values('subject_id').isin([s])] for s in (list(all_test_subjects))] \\\n",
    "#         for df in (X_ext, Y_ext)\n",
    "#     ]\n",
    "\n",
    "    model_hyperparams = copy.copy(hyperparams_fixed)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_fold_hyperparams.items()}\n",
    "    )\n",
    "    batch_size = 16\n",
    "    \n",
    "    # MULTIVARIATE DL\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs_tmp, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop_tmp, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader        = prepare_dataloader(X_test_tmp, Ys_test[t], batch_size=batch_size)\n",
    "    print(\"init dataloaders complete\")\n",
    "    set_primary_seeds(SEED)\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_fold_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model_hyperparams['batch_size'] = batch_size\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_fold_hyperparams.items() if k in ('learning_rate', 'num_epochs', 'patience', 'early_stop_frac')}\n",
    "    )\n",
    "    #model = model.to(torch.device('cuda'))\n",
    "    print(\"Hyperparam: \", model_hyperparams)\n",
    "    best_model, _ = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in model_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        set_primary_seeds(SEED)\n",
    "        probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "        y_scores      = np.concatenate(probabilities_test)\n",
    "        y_true        = np.concatenate(labels_test)\n",
    "        subject_idx = list(range(0,len(y_scores)))\n",
    "        print(\"y_true:\", y_true.shape)\n",
    "        print(\"y_scores:\", y_scores.shape)\n",
    "        print(\"External validation testing for our best model: %s, on target %s\" % (model_name, t))\n",
    "        if output_last:\n",
    "            y_pred = []\n",
    "            m = nn.Softmax(dim=1)\n",
    "            y_scores_sm = m(torch.from_numpy(y_scores))\n",
    "            y_pred = np.argmax(y_scores_sm, 1).numpy()\n",
    "            y_score = y_scores_sm.numpy()[:,1]\n",
    "            print(\"y_score:\",y_score.shape)\n",
    "            auc = roc_auc_score(y_true, y_score)\n",
    "            auprc = average_precision_score(y_true, y_score)\n",
    "            acc   = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred)\n",
    "            rec = recall_score(y_true, y_pred)\n",
    "            F1    = f1_score(y_true, y_pred)\n",
    "            print(\"auc->%f, auprc->%f, acc->%f, prec->%f, rec->%f, F1->%f\" % (auc, auprc, acc, prec, rec, F1))\n",
    "        else:\n",
    "            mse = mean_squared_error(y_score, labels_test)\n",
    "            rmse = sqrt(mse)\n",
    "            mae = mean_absolute_error(y_score, labels_test)\n",
    "            pred_mean = np.mean(labels_test)\n",
    "            pred_std = np.std(labels_test)\n",
    "            label_mean = np.mean(y_score)\n",
    "            label_std = np.std(y_score)\n",
    "            print(\"mse->%f, rmse->%f, MAE->%f, (pred mean, pred_std)->(%f,%f), (label_mean, label_std)->(%f,%f)\" % (mse, rmse, mae, pred_mean, pred_std, label_mean, label_std))\n",
    "    assert len(y_true) == len(y_score), \"Labels (%d) and y_score lengths (%d) dont match\"%(len(y_true),len(y_score))\n",
    "    preds_ext.append_logger(subject_idx, y_true, y_score, label=t, fold=fold)\n",
    "    print()\n",
    "preds_int.df = preds_df_to_int(preds_int.df)\n",
    "preds_int.df.to_csv(RESULTS_DIR+\"GRU_internal_test_preds.csv\")\n",
    "preds_ext.df = preds_df_to_int(preds_ext.df)\n",
    "preds_ext.df.to_csv(RESULTS_DIR+\"GRU_external_test_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write cell output (text) from above into a dedicate logfile\n",
    "'''\n",
    "write_file = RESULTS_DIR + 'GRU_main_output_hyperparam_sweep_val.txt'\n",
    "with open(write_file, 'w') as f:\n",
    "    f.write(cap.stdout)\n",
    "del(cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_dev)",
   "language": "python",
   "name": "pytorch_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
