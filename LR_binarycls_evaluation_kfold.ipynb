{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See github directions on getting data access from Physionet.org\n",
    "'''\n",
    "DATA_FILEPATH     = \"./data/all_hourly_data.h5\"\n",
    "#RAW_DATA_FILEPATH = './all_hourly_data.h5'\n",
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "RESULTS_DIR     = \"./results/\"\n",
    "GPU               = '2'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def set_primary_seeds(seed):\n",
    "    print(\"Setting primary seeds...\")\n",
    "    if not seed:\n",
    "        seed = 1\n",
    "        \n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "#     np.random.RandomState(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # for multiGPUs.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_primary_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "'''\n",
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_full_lvl2 = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "#data_full_raw  = pd.read_hdf(RAW_DATA_FILEPATH, 'vitals_labs') \n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "\n",
    "“Simple Imputation” scheme outlined in Che et al.:\n",
    "\n",
    "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan\n",
    "Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing\n",
    "Values. Scientific Reports 8, 1 (2018).\n",
    "\n",
    "'''\n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing to define 3-day and 7-day length of stay outcome labels\n",
    "'''\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "# Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2, raw = [df[\n",
    "    (df.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (df.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "] for df in (data_full_lvl2, data_full_lvl2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subjects that were admited before and after 2180 [2100,2200]\n",
    "Ys['admittime'] = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['admittime']]\n",
    "split_date = np.datetime64('2180-01-01')\n",
    "Ys_int = Ys[Ys['admittime']<split_date]\n",
    "Ys_ext = Ys[Ys['admittime']>=split_date]\n",
    "subjects_int = Ys_int.index\n",
    "subjects_ext = Ys_ext.index\n",
    "del(Ys_int)\n",
    "del(Ys_ext)\n",
    "Ys.drop(labels='admittime',axis=1,inplace=True)\n",
    "Ys.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl2_subj_idx,  Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, Ys)]\n",
    "#lvl2_subj_idx, raw_subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, raw, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "#assert lvl2_subjects == set(raw_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "# shuffle the dataset\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "\n",
    "# standardize the whole dataset\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2.loc[:, idx[:,'mean']].mean(axis=0), lvl2.loc[:, idx[:,'mean']].std(axis=0)\n",
    "lvl2.loc[:, idx[:,'mean']] = (lvl2.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "\n",
    "# impute missing values\n",
    "lvl2 = simple_imputer(lvl2)\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "X_int = lvl2.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "X_ext = lvl2.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "Y_int = Ys.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "Y_ext = Ys.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))\n",
    "\n",
    "all_int_subjects = list(\n",
    "    np.random.permutation(Y_int.index.get_level_values('subject_id').values)\n",
    ")\n",
    "\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_train, Ys_train, X_dev, Ys_dev, X_test, Ys_test, target):\n",
    "    best_s, best_hyperparams, best_hyperparams_i = -np.Inf, None, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "        M = model(**hyperparams)\n",
    "        M = M.fit(X_train, Ys_train[target])\n",
    "        y_score = M.predict_proba(X_dev)[:, 1]\n",
    "        s = roc_auc_score(Ys_dev[target], y_score)\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams, best_hyperparams_i  = s, hyperparams, i\n",
    "            print(\"New Best AUC Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "    return run_only_final(model, best_hyperparams, best_hyperparams_i, X_train, Ys_train, X_dev, Ys_dev, X_test, Ys_test, target)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, best_hyperparams_i, X_train, Ys_train, X_dev, Ys_dev, X_test, Ys_test, target, pass_thru_model=False):\n",
    "    if not pass_thru_model:\n",
    "        best_M = model(**best_hyperparams)\n",
    "    else:\n",
    "        best_M = model\n",
    "        \n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "    print(Ys_train.shape)\n",
    "    print(Ys_dev.shape)\n",
    "    best_M.fit(pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))[target])\n",
    "    y_true  = Ys_test[target]\n",
    "    y_score = best_M.predict_proba(X_test)[:, 1]\n",
    "    y_pred  = best_M.predict(X_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return best_M, best_hyperparams, best_hyperparams_i, auc, auprc, acc, F1, y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some helper data structures to store and save predictions\n",
    "'''\n",
    "class Logger():\n",
    "    def __init__(self, optional_cols=None):\n",
    "        #self.n_samples=n_samples\n",
    "        self.columns=['task_name','fold','prediction_no','index','y_true','y_score','censoring']\n",
    "        if (optional_cols is None):\n",
    "            self.df=pd.DataFrame(columns=self.columns)\n",
    "            self.has_optional_cols=False\n",
    "        else:\n",
    "            self.df=pd.DataFrame(columns=self.columns+optional_cols)\n",
    "            self.has_optional_cols=True\n",
    "            self.optional_cols=optional_cols\n",
    "        self._rocs=[]\n",
    "        self._prediction_no=0\n",
    "        return\n",
    "    \n",
    "    def append_logger(self,indices, y_true, y_score, label, censoring=None, optional_dict=None,fold=0):\n",
    "        y_true=np.array(y_true).astype(int)\n",
    "        y_score=np.array(y_score).astype(float)\n",
    "        \n",
    "\n",
    "        if ((y_true.shape[0]!=y_score.shape[0])):\n",
    "            raise ValueError(\"Shapes of input matrices must match\")\n",
    "        \n",
    "            \n",
    "        self._n=y_true.shape[0]\n",
    "\n",
    "        if(censoring is None):\n",
    "            cens = self._n*[math.nan]\n",
    "            censoring=np.array(censoring)\n",
    "        else:\n",
    "            cens=censoring\n",
    "        \n",
    "        arr=np.array([self._n*[label],\n",
    "                      self._n*[fold],\n",
    "                      self._n*[self._prediction_no],\n",
    "                      list(indices),\n",
    "                      list(y_true),\n",
    "                      list(y_score),\n",
    "                      list(cens)\n",
    "              ]).transpose()\n",
    "\n",
    "        to_append=pd.DataFrame(arr, columns=self.columns)\n",
    "        if(self.has_optional_cols):\n",
    "            \n",
    "            for column, value in optional_dict.items():\n",
    "                to_append.loc[:,column]=value\n",
    "\n",
    "        self.df=self.df.append(to_append)\n",
    "        self._prediction_no=self._prediction_no+1\n",
    "        \n",
    "def preds_df_to_int(df):\n",
    "    df_test = df\n",
    "    type_dict = {}\n",
    "    cast=['fold','prediction_no','y_true']\n",
    "    for col in cast:\n",
    "        type_dict[col] = 'int64'\n",
    "    df_test = df_test.astype(dtype=type_dict)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3)),\n",
    "    'penalty': Choice(['l1', 'l2']),\n",
    "    'solver': Choice(['liblinear']),\n",
    "    'max_iter': Choice([100])\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "for i in range(N):\n",
    "    if LR_hyperparams_list[i]['solver'] == 'lbfgs': LR_hyperparams_list[i]['penalty'] = 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_hyperparams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR     = \"/home/bryanbed/Projects/DevNotebooks/results/\"\n",
    "#with open(RESULTS_PATH, mode='rb') as f: results = pickle.load(f)\n",
    "#RESULTS_PATH = './results/regression_baseline_res.pkl'\n",
    "RERUN = True\n",
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo LogisticRegression Classifier with LOS_3\n",
    "# auc, auprc, acc, F1\n",
    "early_stop_frac = 0.1\n",
    "sss = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "outcomes = ['los_3', 'los_7', 'mort_icu', 'mort_hosp']\n",
    "model_name = 'LR'\n",
    "model = LogisticRegression\n",
    "hyperparams_list = LR_hyperparams_list\n",
    "preds_int = Logger()\n",
    "preds_ext = Logger()\n",
    "for t in outcomes:\n",
    "    print(\"Outcome:\", t)\n",
    "    Y_int[t] = Y_int[t].astype(int)\n",
    "    Y_ext[t] = Y_ext[t].astype(int)\n",
    "    fold=1\n",
    "    best_fold_F1, best_fold_rmse, best_fold_auc, best_fold_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "    best_fold_model_name = \"N/A\"\n",
    "    best_fold = -1\n",
    "    best_fold_model = None\n",
    "    for train_index, test_index in sss.split(idx[subjects_int.get_level_values('subject_id')], Y_int[t]):\n",
    "        # Internal: 10-fold cross validation for training split\n",
    "        best_F1, best_rmse, best_auc, best_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "        best_hyperparams = None\n",
    "        best_preds = []\n",
    "        best_model = None\n",
    "        print(\"Evaluating %s for Outcome: %s, Fold: %d\"%(model_name,t,fold))\n",
    "        #train_data = subjects\n",
    "        train_subj = subjects_int[train_index].get_level_values('subject_id')\n",
    "        test_subj = subjects_int[test_index].get_level_values('subject_id')\n",
    "        [(X_train, X_test), (Ys_train, Ys_test)] = [\n",
    "            [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj,  test_subj)] \\\n",
    "            for df in (X_int, Y_int)\n",
    "        ]\n",
    "        for df in X_train, X_test, Ys_train, Ys_test: assert not df.isnull().any().any()\n",
    "            \n",
    "        set_primary_seeds(SEED)\n",
    "        all_train_subjects = list(\n",
    "            np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "        )\n",
    "        N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "        train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "        early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "\n",
    "        X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        Ys_train_early_stop = Ys_train[Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        \n",
    "        print(\"Train subjects length: \", len(train_subjects))\n",
    "        print(\"ES/valid subjects length: \", len(early_stop_subjects))   \n",
    "        print(\"Test subjects length: \", len(test_subj))\n",
    "              \n",
    "        print(\"Running model %s on target %s\" % (model_name, t))\n",
    "        \n",
    "        print(X_train_obs.shape)\n",
    "\n",
    "        X_train_obs, X_train_early_stop, X_test = [\n",
    "            df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "                X_train_obs, X_train_early_stop, X_test\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        best_model, best_hyperparams_int, best_hyperparams_i, best_auc, auprc, acc, F1, y_true, y_score = run_basic(\n",
    "            model, hyperparams_list, X_train_obs, Ys_train_obs, X_train_early_stop, Ys_train_early_stop, X_test, Ys_test, t\n",
    "        )\n",
    "        print(\"Final results for model %s on outcome %s\" % (model_name, t))\n",
    "        print(\"auc->%f, auprc->%f, acc->%f, F1->%f\" % (best_auc, auprc, acc, F1))\n",
    "\n",
    "        print(\"New Best AUC within fold %s: %.2f @ hyperparams = %s\" % (str(fold), 100*best_auc, repr((best_hyperparams_int))))\n",
    "#         best_true = y_true\n",
    "#         best_preds = y_pred\n",
    "              \n",
    "        if best_auc > best_fold_auc:\n",
    "            best_fold_auc, best_fold_hyperparams = best_auc, best_hyperparams_int\n",
    "#             best_fold_model_name = model_name\n",
    "            print(\"New Best AUC across all folds: %.2f @ hyperparams = %s\" % (100*best_fold_auc, repr((best_fold_hyperparams))))\n",
    "#             save our best model just in case we want it later\n",
    "#             torch.save(best_model.module.state_dict(), 'results/best_overall_model.pt')\n",
    "            best_fold = fold\n",
    "            best_fold_model = best_model\n",
    "        assert len(y_true) == len(y_score), \"Labels (%d) and preds lengths (%d) dont match\"%(len(y_true),len(y_score))\n",
    "        print(\"Appending best predictions from Fold %d. Length: %d\"%(fold, len(y_score)))\n",
    "        subject_idx = list(range(0,len(y_true)))\n",
    "        preds_int.append_logger(subject_idx, y_true, y_score, label=t, fold=fold)\n",
    "        fold+=1\n",
    "        print()\n",
    "    # external\n",
    "    set_primary_seeds(SEED)\n",
    "    all_train_subjects = list(\n",
    "        np.random.permutation(Y_int.index.get_level_values('subject_id').values)\n",
    "    )\n",
    "    N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "    early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "\n",
    "    X_train_obs         = X_int[X_int.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Y_int[Y_int.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    X_train_early_stop  = X_int[X_int.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Y_int[Y_int.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    X_test = X_ext\n",
    "    Ys_test = Y_ext\n",
    "\n",
    "    X_train_obs, X_train_early_stop, X_test = [\n",
    "        df.pivot_table(index=['subject_id', 'hadm_id', 'icustay_id'], columns=['hours_in']) for df in (\n",
    "            X_train_obs, X_train_early_stop, X_test\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    _, _, _, auc, auprc, acc, F1, y_true, y_score = run_only_final(\n",
    "        best_fold_model, best_fold_hyperparams, 1, X_train_obs, Ys_train_obs, X_train_early_stop, Ys_train_early_stop, X_test, Ys_test, t, pass_thru_model=True\n",
    "    )\n",
    "    print(\"External Test Results: auc->%f, auprc->%f, acc->%f, F1->%f\" % (auc, auprc, acc, F1))\n",
    "    assert len(y_true) == len(y_score), \"Labels (%d) and preds lengths (%d) dont match\"%(len(y_true),len(y_score))\n",
    "    print(\"Appending best overall predictions. Length: %d\"%(len(y_score)))\n",
    "    subject_idx = list(range(0,len(y_true)))\n",
    "    preds_ext.append_logger(subject_idx, y_true, y_score, label=t, fold=fold)\n",
    "    print()\n",
    "\n",
    "preds_int.df = preds_df_to_int(preds_int.df)\n",
    "preds_int.df.to_csv(RESULTS_DIR+\"LR_internal_test_preds.csv\")\n",
    "preds_ext.df = preds_df_to_int(preds_ext.df)\n",
    "preds_ext.df.to_csv(RESULTS_DIR+\"LR_external_test_preds.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_dev)",
   "language": "python",
   "name": "pytorch_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
