{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import copy, math, os, pickle, time, sys, os, random, argparse, pandas as pd, numpy as np, scipy.stats as ss\n",
    "import pathlib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "import multiprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single CUDA device, comment for directions on multiple GPU\n",
    "'''\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See CUDA versions, CUDA must be available for GPU acceleration\n",
    "'''\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Single CUDA device, comment for directions on multiple GPU\n",
    "'''\n",
    "torch.cuda.get_device_name(0)\n",
    "# torch.cuda.get_device_name(1)\n",
    "# torch.cuda.get_device_name(2)\n",
    "# torch.cuda.get_device_name(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See github directions on getting data access from Physionet.org\n",
    "'''\n",
    "DATA_FILEPATH     = \"./data/all_hourly_data.h5\"\n",
    "#RAW_DATA_FILEPATH = './all_hourly_data.h5'\n",
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 1\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "RESULTS_DIR     = \"./results/\"\n",
    "PROCESSED_DATA_DIR = \"./processed_data/\"\n",
    "\n",
    "def set_primary_seeds(seed):\n",
    "    print(\"Setting primary seeds...\")\n",
    "    if not seed:\n",
    "        seed = 1\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # for multiple GPUs.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_primary_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "'''\n",
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_full_lvl2 = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some tools from: https://github.com/MLforHealth/MIMIC_Extract/tree/master/notebooks\n",
    "\n",
    "“Simple Imputation” scheme outlined in Che et al.:\n",
    "\n",
    "Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan\n",
    "Liu. 2018. Recurrent Neural Networks for Multivariate Time Series with Missing\n",
    "Values. Scientific Reports 8, 1 (2018).\n",
    "\n",
    "'''\n",
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    df = df.copy()\n",
    "    if len(df.columns.names) > 2: df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]]\n",
    "    icustay_means = df_out.loc[:, idx[:, 'mean']].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,idx[:,'mean']] = df_out.loc[:,idx[:,'mean']].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data preprocessing to define 3-day and 7-day length of stay outcome labels\n",
    "'''\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['mort_hosp', 'mort_icu', 'los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "# Ys.drop(columns=['los_icu'], inplace=True)\n",
    "Ys.astype(float)\n",
    "\n",
    "lvl2, raw = [df[\n",
    "    (df.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (df.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "] for df in (data_full_lvl2, data_full_lvl2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START: LOAD TCN SOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TCN source from https://github.com/locuslab/TCN\n",
    "'''\n",
    "# removes (k-1) elements from the output on the right. Ensures causality. We first pad and then chomp.\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TCN source from https://github.com/locuslab/TCN\n",
    "'''\n",
    "# a single temporal layer/block\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TCN source from https://github.com/locuslab/TCN\n",
    "'''\n",
    "\n",
    "# Create different layers with different dilation sizes\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "#         self.output_hook = OutputHook()\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            next_block = TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)\n",
    "#             next_block.relu1.register_forward_hook(self.output_hook)\n",
    "#             next_block.relu2.register_forward_hook(self.output_hook)\n",
    "            layers += [next_block]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TCN source from https://github.com/locuslab/TCN\n",
    "'''\n",
    "\n",
    "# a TCN mode with softmax for classification\n",
    "# input size = number of features\n",
    "# output size = 2\n",
    "# num_channels = nhid\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N -> \n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "#         print(\"Input shape 1\")\n",
    "#         print(x.shape)\n",
    "#         print(torch.transpose(x,0,1).shape)\n",
    "#         output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        #x = torch.transpose(x, 0, 1)\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "#         print(\"Input shape 2\")\n",
    "#         print(x.shape)\n",
    "#         print(x.shape)\n",
    "#                 output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "#         output = self.tcn(x)\n",
    "#         output = self.linear(output).double()\n",
    "        x = x.transpose(1, 2)\n",
    "#         print(\"Input shape 3\")\n",
    "#         print(x.shape)\n",
    "\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        output = self.tcn(x).transpose(1, 2)\n",
    "        output = self.linear(output).double()\n",
    "#         print(\"Output shape\")\n",
    "#         print(output.shape)\n",
    "#         print(\"output opt1: \", F.log_softmax(output))\n",
    "#         print(\"output opt2: \", F.log_softmax(output,dim=1))\n",
    "        #return output\n",
    "\n",
    "    \n",
    "        return self.sig(output), output, self.tanh(output)\n",
    "        #return self.sig(output)\n",
    "        #return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "See inpute features (mean and std used for normalization)\n",
    "'''\n",
    "list(data_full_lvl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get subjects that were admited before and after 2180 [2100,2200]\n",
    "# This selected to provide split for simulation prospective study. Years encoded during anonymization per MIMIC-Extract\n",
    "Ys['admittime'] = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['admittime']]\n",
    "split_date = np.datetime64('2180-01-01')\n",
    "Ys_int = Ys[Ys['admittime']<split_date]\n",
    "Ys_ext = Ys[Ys['admittime']>=split_date]\n",
    "subjects_int = Ys_int.index\n",
    "subjects_ext = Ys_ext.index\n",
    "del(Ys_int)\n",
    "del(Ys_ext)\n",
    "Ys.drop(labels='admittime',axis=1,inplace=True)\n",
    "Ys.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "More data processing\n",
    "'''\n",
    "# standardize and impute\n",
    "lvl2_subj_idx,  Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (lvl2, Ys)]\n",
    "lvl2_subjects = set(lvl2_subj_idx)\n",
    "assert lvl2_subjects == set(Ys_subj_idx), \"Subject ID pools differ!\"\n",
    "\n",
    "# shuffle the dataset\n",
    "subjects, N = np.random.permutation(list(lvl2_subjects)), len(lvl2_subjects)\n",
    "\n",
    "# standardize the whole dataset\n",
    "idx = pd.IndexSlice\n",
    "lvl2_means, lvl2_stds = lvl2.loc[:, idx[:,'mean']].mean(axis=0), lvl2.loc[:, idx[:,'mean']].std(axis=0)\n",
    "lvl2.loc[:, idx[:,'mean']] = (lvl2.loc[:, idx[:,'mean']] - lvl2_means)/lvl2_stds\n",
    "\n",
    "# impute missing values\n",
    "lvl2 = simple_imputer(lvl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"external\" is a bad term for the test set in simulated prospective\n",
    "'''\n",
    "### Split internal and sets datasets\n",
    "idx = pd.IndexSlice\n",
    "X_int = lvl2.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "X_ext = lvl2.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "Y_int = Ys.loc[idx[subjects_int.get_level_values('subject_id')],:]\n",
    "Y_ext = Ys.loc[idx[subjects_ext.get_level_values('subject_id')],:]\n",
    "\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))\n",
    "\n",
    "all_int_subjects = list(\n",
    "    np.random.permutation(Y_int.index.get_level_values('subject_id').values)\n",
    ")\n",
    "print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some helper data structures to store and save predictions\n",
    "'''\n",
    "class Logger():\n",
    "    def __init__(self, optional_cols=None):\n",
    "        #self.n_samples=n_samples\n",
    "        self.columns=['task_name','fold','prediction_no','index','y_true','y_score','censoring']\n",
    "        if (optional_cols is None):\n",
    "            self.df=pd.DataFrame(columns=self.columns)\n",
    "            self.has_optional_cols=False\n",
    "        else:\n",
    "            self.df=pd.DataFrame(columns=self.columns+optional_cols)\n",
    "            self.has_optional_cols=True\n",
    "            self.optional_cols=optional_cols\n",
    "        self._rocs=[]\n",
    "        self._prediction_no=0\n",
    "        return\n",
    "    \n",
    "    def append_logger(self,indices, y_true, y_score, label, censoring=None, optional_dict=None,fold=0):\n",
    "        y_true=np.array(y_true).astype(int)\n",
    "        y_score=np.array(y_score).astype(float)\n",
    "        \n",
    "\n",
    "        if ((y_true.shape[0]!=y_score.shape[0])):\n",
    "            raise ValueError(\"Shapes of input matrices must match\")\n",
    "        \n",
    "            \n",
    "        self._n=y_true.shape[0]\n",
    "\n",
    "        if(censoring is None):\n",
    "            cens = self._n*[math.nan]\n",
    "            censoring=np.array(censoring)\n",
    "        else:\n",
    "            cens=censoring\n",
    "        \n",
    "        arr=np.array([self._n*[label],\n",
    "                      self._n*[fold],\n",
    "                      self._n*[self._prediction_no],\n",
    "                      list(indices),\n",
    "                      list(y_true),\n",
    "                      list(y_score),\n",
    "                      list(cens)\n",
    "              ]).transpose()\n",
    "\n",
    "        to_append=pd.DataFrame(arr, columns=self.columns)\n",
    "        if(self.has_optional_cols):\n",
    "            \n",
    "            for column, value in optional_dict.items():\n",
    "                to_append.loc[:,column]=value\n",
    "\n",
    "        self.df=self.df.append(to_append)\n",
    "        self._prediction_no=self._prediction_no+1\n",
    "        \n",
    "def preds_df_to_int(df):\n",
    "    df_test = df\n",
    "    type_dict = {}\n",
    "    cast=['fold','prediction_no','y_true']\n",
    "    for col in cast:\n",
    "        type_dict[col] = 'int64'\n",
    "    df_test = df_test.astype(dtype=type_dict)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary training method for the TCN model\n",
    "'''\n",
    "def TCN_Train_Model(\n",
    "    model, train_dataloader, valid_dataloader, num_epochs = 300, patience = 3, min_delta = 1e-7, learning_rate=1e-3, lr_deg=1, batch_size=64,\\\n",
    "    clip=0, loss_function=1, clamp=0, activity_reg=0,l2_penalty_bool=0, l2_penalty_val=0., amsgrad=0, output_last=True\n",
    "):\n",
    "    \n",
    "    print('Start Training ... ')\n",
    "\n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_nll = torch.nn.NLLLoss()\n",
    "    loss_CEL = torch.nn.CrossEntropyLoss()\n",
    "    loss_L1 = torch.nn.L1Loss()\n",
    "    loss_BCE = torch.nn.BCELoss()\n",
    "    loss_BCE_logits = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_hinge = torch.nn.HingeEmbeddingLoss()\n",
    "    \n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    \n",
    "#     learning_rate = 0.001\n",
    "#     optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, alpha=0.99)\n",
    "#     use_gpu = False#torch.cuda.is_available()\n",
    "    \n",
    "    # weight decay seems broken, returning nans (06/21/21)\n",
    "    l2_penalty=0.0\n",
    "    if l2_penalty_bool:\n",
    "        l2_penalty = l2_penalty_val\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        model = model.to(device)\n",
    "        use_gpu = True\n",
    "        if torch.cuda.device_count()==1:\n",
    "            print(\"CUDA is available, 1 device recognized\")\n",
    "#             model = model.to(device)\n",
    "        else:\n",
    "            model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "            #model = nn.parallel.DistributedDataParallel(model, device_ids=[0,1,2,3])\n",
    "            print(\"CUDA is available, multiple devices recognized\")\n",
    "    else:\n",
    "        #cuda_available = 0\n",
    "        use_gpu = False\n",
    "        device = 'cpu'\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably change cuda_available to 1\")\n",
    "    \n",
    "    interval = 100\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    losses_epochs_train = []\n",
    "    losses_epochs_valid = []\n",
    "    \n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "    \n",
    "    # Variables for Early Stopping\n",
    "    is_best_model = 0\n",
    "    best_model = model\n",
    "    patient_epoch = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        trained_number = 0\n",
    "\n",
    "        lr = learning_rate * (lr_deg) ** epoch # lr_deg 0 < x < inf\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_penalty, amsgrad=amsgrad)\n",
    "        \n",
    "        losses_epoch_train = []\n",
    "        losses_epoch_valid = []\n",
    "        \n",
    "        valid_dataloader_iter = iter(valid_dataloader)\n",
    "        \n",
    "        for batch_idx, (X, labels) in enumerate(train_dataloader):\n",
    "            assert X.size()[0] == batch_size, \"Batch Size doesn't match! %s\" % str(X.size())\n",
    "            if use_gpu: X, labels = X.to(device), labels.to(device)\n",
    "            # pretty sure torch.autograd.Variable() is depreciated but should just return a tensor\n",
    "            X, labels = Variable(X), Variable(labels)\n",
    "            model.zero_grad()\n",
    "            prediction_sig,prediction_noact,prediction_tanh=model(X)\n",
    "            if output_last:\n",
    "\n",
    "                if loss_function == 1:\n",
    "                    prediction = torch.flatten(prediction_noact)\n",
    "                    loss_train = loss_BCE_logits(prediction.double(), labels.double())\n",
    "                elif loss_function == 2:\n",
    "                    prediction = torch.flatten(prediction_tanh)\n",
    "                    loss_train = loss_hinge(prediction.double(), labels.double())\n",
    "            else:\n",
    "#                 full_labels = torch.cat((inputs[:,1:,:], labels), dim = 1)\n",
    "#                 loss_train = loss_MSE(outputs, full_labels)\n",
    "                prediction = torch.flatten(prediction_noact)\n",
    "                loss_train = loss_MSE(prediction.double(), labels.double())\n",
    "\n",
    "                \n",
    "            losses_train.append(loss_train.data)\n",
    "            losses_epoch_train.append(loss_train.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            \n",
    "            if clip > 1.0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            if clamp:\n",
    "                with torch.no_grad():\n",
    "                    for param in model.parameters():\n",
    "                        param.clamp_(-1, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            try: \n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "            except:\n",
    "                del valid_dataloader_iter\n",
    "                valid_dataloader_iter = iter(valid_dataloader)\n",
    "                X_val, labels_val = next(valid_dataloader_iter)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            if use_gpu: X_val, labels_val = X_val.to(device), labels_val.to(device)\n",
    "\n",
    "            prediction_val_sig,prediction_val_noact,prediction_val_tanh = model(X_val)\n",
    "            \n",
    "            if output_last:\n",
    "\n",
    "                if loss_function == 1:\n",
    "                    prediction = torch.flatten(prediction_val_noact)\n",
    "                    loss_valid = loss_BCE_logits(prediction.double(), labels_val.double())\n",
    "                elif loss_function == 2:\n",
    "                    prediction = torch.flatten(prediction_val_tanh)\n",
    "                    loss_valid = loss_hinge(prediction.double(), labels_val.double())\n",
    "            else:\n",
    "#                 raise NotImplementedError(\"Should be output last!\")\n",
    "#                 full_labels_val = torch.cat((inputs_val[:,1:,:], labels_val), dim = 1)\n",
    "#                 loss_valid = loss_MSE(outputs_val, full_labels_val)\n",
    "                prediction = torch.flatten(prediction_val_noact)\n",
    "                loss_valid = loss_MSE(prediction.double(), labels_val.double())\n",
    "            \n",
    "            losses_valid.append(loss_valid.data)\n",
    "            losses_epoch_valid.append(loss_valid.data)\n",
    "            trained_number += 1\n",
    "            \n",
    "        avg_losses_epoch_train = sum(losses_epoch_train).cpu().numpy() / float(len(losses_epoch_train))\n",
    "        avg_losses_epoch_valid = sum(losses_epoch_valid).cpu().numpy() / float(len(losses_epoch_valid))\n",
    "        losses_epochs_train.append(avg_losses_epoch_train)\n",
    "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
    "        \n",
    "\n",
    "        # Early Stopping\n",
    "        if epoch == 0:\n",
    "            is_best_model = 1\n",
    "            best_model = model\n",
    "            #torch.save(model.module.state_dict(), 'results/best_model.pt')\n",
    "            min_loss_epoch_valid = 10000.0\n",
    "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "        else:\n",
    "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
    "                is_best_model = 1\n",
    "                best_model = model\n",
    "                #torch.save(model.module.state_dict(), 'results/best_model.pt')\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid \n",
    "                patient_epoch = 0\n",
    "            else:\n",
    "                is_best_model = 0\n",
    "                patient_epoch += 1\n",
    "                if patient_epoch >= patience:\n",
    "                    print('Early Stopped at Epoch:', epoch)\n",
    "                    break\n",
    "    \n",
    "        # Print training parameters\n",
    "        cur_time = time.time()\n",
    "        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n",
    "                    epoch, \\\n",
    "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
    "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
    "                    np.around([cur_time - pre_time] , decimals=2),\\\n",
    "                    is_best_model) )\n",
    "        pre_time = cur_time\n",
    "\n",
    "        \n",
    "    del valid_dataloader_iter\n",
    "                \n",
    "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]\n",
    "#     return [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Primary validation method for the TCN model\n",
    "'''\n",
    "def predict_proba(model, dataloader, loss_function, output_last=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        model: TCN model\n",
    "        test_dataloader: containing batches of measurement, measurement_last_obsv, mask, time_, labels\n",
    "    Returns:\n",
    "        predictions: size[num_samples, 2]\n",
    "        labels: size[num_samples]\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "\n",
    "    #cuda_available = 0\n",
    "    if torch.cuda.is_available():\n",
    "        use_gpu = True\n",
    "#         if torch.cuda.device_count()==1:\n",
    "#             print(\"CUDA is available, 1 device recognized\")\n",
    "#         else:\n",
    "#             model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "#             print(\"CUDA is available, multiple devices recognized\")\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        #cuda_available = 0\n",
    "        use_gpu = False\n",
    "#         print(\"WARNING: You have a CUDA device, so you should probably change cuda_available to 1\")\n",
    "    \n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    probabilities = []\n",
    "    labels        = []\n",
    "    ethnicities   = []\n",
    "    genders       = []\n",
    "    ids   = []\n",
    "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
    "        if use_gpu: X, Y = X.to(device), Y.to(device)\n",
    "        # pretty sure torch.autograd.Variable() is depreciated but should just return a tensor\n",
    "        X, Y = Variable(X), Variable(Y)\n",
    "\n",
    "        pred_sig, pred_noact, pred_tanh = model(X)\n",
    "        \n",
    "        if output_last:\n",
    "            if loss_function == 1:\n",
    "                pred = torch.flatten(pred_sig)\n",
    "            elif loss_function == 2:\n",
    "                pred = torch.flatten(pred_tanh)\n",
    "        else:\n",
    "            pred = torch.flatten(pred_noact)\n",
    "            \n",
    "        next_pred = pred.detach().cpu().data.numpy()\n",
    "        next_lab = Y.detach().cpu().data.numpy()\n",
    "\n",
    "        probabilities.append(next_pred)\n",
    "        #probabilities.append(pred.cpu().data.numpy())\n",
    "        labels.append(next_lab)\n",
    "    \n",
    "    return probabilities, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch dataloader and associated tools\n",
    "'''\n",
    "def to_3D_tensor(df):\n",
    "    idx = pd.IndexSlice\n",
    "    return np.dstack((df.loc[idx[:,:,:,i], :].values for i in sorted(set(df.index.get_level_values('hours_in')))))\n",
    "\n",
    "def to_2D_tensor(df):\n",
    "    dl = list()\n",
    "    for r_idx, row in df.iterrows():\n",
    "        d = df.loc[r_idx,'data']\n",
    "        dl.append(d)\n",
    "    return np.stack(tuple(dl))\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def prepare_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_3D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    return utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=shuffle, drop_last = True)\n",
    "\n",
    "def prepare_2d_dataloader(df, Ys, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    dfs = (df_train, df_dev, df_test).\n",
    "    df_* = (subject, hadm, icustay, hours_in) X (level2, agg fn \\ni {mask, mean, time})\n",
    "    Ys_series = (subject, hadm, icustay) => label.\n",
    "    \"\"\"\n",
    "    X     = torch.from_numpy(to_2D_tensor(df).astype(np.float32))\n",
    "    label = torch.from_numpy(Ys.values.astype(np.int64))\n",
    "    dataset = utils.TensorDataset(X, label)\n",
    "    return utils.DataLoader(dataset, batch_size=int(batch_size), shuffle=shuffle, drop_last = True)\n",
    "\n",
    "\n",
    "def stack_dataframe(df):\n",
    "    df.columns = df.columns.map('_'.join)\n",
    "    df2 = pd.DataFrame(index=df.index.droplevel(level=3).drop_duplicates())\n",
    "    df2['data'] = ''\n",
    "    for idx, df in df.groupby(level=[0,1,2]):\n",
    "        data = []\n",
    "        cols = list(df)\n",
    "        n = len(df[cols[0]])\n",
    "        for i in range(n):\n",
    "            for col in cols:\n",
    "                k = len(df[col])\n",
    "                if n != k:\n",
    "                    print(\"different len: \", n, \"vs. \", k)\n",
    "                #print(df[col].iloc[i])\n",
    "                data.append(df[col].iloc[i])\n",
    "        df2.loc[idx,'data'] = data\n",
    "        #print(df2.shape)\n",
    "    return df2\n",
    "\n",
    "def stack_dataframe_alt(df):\n",
    "    df.columns = df.columns.map('_'.join)\n",
    "    df2 = pd.DataFrame(index=df.index.droplevel(level=3).drop_duplicates())\n",
    "    df2['data'] = ''\n",
    "    for idx, df in df.groupby(level=[0,1,2]):\n",
    "        data = []\n",
    "        cols = list(df)\n",
    "        n = len(df[cols[0]])\n",
    "        for col in cols:\n",
    "            tmp_array = df[col]\n",
    "            for val in tmp_array:\n",
    "                data.append(val)\n",
    "        df2.loc[idx,'data'] = data\n",
    "    return df2\n",
    "\n",
    "def ps_spawn(i, shared_dict, df):\n",
    "    res = stack_dataframe_alt(df)\n",
    "    shared_dict[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "num_features = 312\n",
    "input_channels = 24*num_features # 1 for 1D data, 2 for 2D Data\n",
    "n_classes = 1\n",
    "early_stop_frac = 0.1\n",
    "\n",
    "'''\n",
    "Define fixed hyperparameters\n",
    "'''\n",
    "TCN_hyperparams_fixed = {\n",
    "    'input_channels': input_channels, # 1 for 1D data, 2 for 2D Data\n",
    "    'n_classes': n_classes,\n",
    "    'early_stop_frac': early_stop_frac,\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "'''\n",
    "Define search space for hyperparameter grid search\n",
    "'''\n",
    "\n",
    "# Search space from publication: https://www.nature.com/articles/s41598-022-25472-z\n",
    "'''\n",
    "dropout_list = [0.8561711076089411] # between 0.80 to 0.85\n",
    "levels_list = [10, 12]\n",
    "nhid_list = [250,209]\n",
    "kernel_size_list = [5,7]\n",
    "num_epochs_list = [100]\n",
    "patience_list = [3]\n",
    "learning_rate_list = [3.0e-5,5.0e-5,7e-5]\n",
    "lr_deg_list=[1.4979573737643916]\n",
    "batch_size_list = [128]\n",
    "loss_function_list = [1]\n",
    "clamp_list = [1]\n",
    "clip_list = [1.3773119536842078]\n",
    "activity_reg_list = [0]\n",
    "l2_penalty_bool_list = [0]\n",
    "l2_penalty_val_list = [0.06341248051564571]\n",
    "ams_grad_list = [1]\n",
    "'''\n",
    "\n",
    "# single combination for testing and dev.\n",
    "dropout_list = [0.8561711076089411] # between 0.80 to 0.85\n",
    "levels_list = [10]\n",
    "nhid_list = [250]\n",
    "kernel_size_list = [5]\n",
    "num_epochs_list = [100]\n",
    "patience_list = [3]\n",
    "learning_rate_list = [3.0e-5]\n",
    "lr_deg_list=[1.4979573737643916]\n",
    "batch_size_list = [128]\n",
    "loss_function_list = [1]\n",
    "clamp_list = [1]\n",
    "clip_list = [1.3773119536842078]\n",
    "activity_reg_list = [0]\n",
    "l2_penalty_bool_list = [0]\n",
    "l2_penalty_val_list = [0.06341248051564571]\n",
    "ams_grad_list = [1]\n",
    "\n",
    "TCN_hyperparams_list = []\n",
    "\n",
    "for dropout in dropout_list:\n",
    "    for levels in levels_list:\n",
    "        for nhid in nhid_list:\n",
    "            for kernel_size in kernel_size_list:\n",
    "                for num_epochs in num_epochs_list:\n",
    "                    for patience in patience_list:\n",
    "                        for learning_rate in learning_rate_list:\n",
    "                            for lr_deg in lr_deg_list:\n",
    "                                for batch_size in batch_size_list:\n",
    "                                    for loss_function in loss_function_list:\n",
    "                                        for clamp in clamp_list:\n",
    "                                            for clip in clip_list:\n",
    "                                                for activity_reg in activity_reg_list:\n",
    "                                                    for l2_penalty_bool in l2_penalty_bool_list:\n",
    "                                                        for l2_penalty_val in l2_penalty_val_list:\n",
    "                                                            for ams_grad in ams_grad_list:\n",
    "                                                                next_params = {'dropout': dropout, 'levels': levels, 'nhid': nhid, 'kernel_size': kernel_size, \\\n",
    "                                                                               'num_epochs': num_epochs, 'patience': patience, 'learning_rate': learning_rate, 'lr_deg':lr_deg,\\\n",
    "                                                                               'batch_size': batch_size, 'loss_function': loss_function, 'clamp': clamp, 'clip': clip, 'activity_reg': activity_reg,\\\n",
    "                                                                               'l2_penalty_bool': l2_penalty_bool, 'l2_penalty_val': l2_penalty_val, 'ams_grad': ams_grad}\n",
    "                                                                TCN_hyperparams_list.append(next_params)\n",
    "print(\"Number of hyperparam configurations: \", len(TCN_hyperparams_list))                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Capture output and save to a text file\n",
    "'''\n",
    "#%%capture cap --no-stderr\n",
    "hyperparams_fixed = TCN_hyperparams_fixed\n",
    "hyperparams_list = TCN_hyperparams_list\n",
    "sss = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "# evaluate for all four BC outcomes. Can remove any entry from this list\n",
    "outcomes = ['los_3', 'los_7', 'mort_icu', 'mort_hosp']\n",
    "preds_int = Logger()\n",
    "preds_ext = Logger()\n",
    "for t in outcomes:\n",
    "    print(\"Outcome:\", t)\n",
    "    int_save_str = RESULTS_DIR + '10Fold_TCN_int_' + str(t)\n",
    "    ext_save_str = RESULTS_DIR + '10Fold_TCN_ext_' + str(t)\n",
    "    fold=1\n",
    "    best_fold_F1, best_fold_rmse, best_fold_auc, best_fold_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "    best_fold_model_name = \"N/A\"\n",
    "    best_fold = -1\n",
    "    for train_index, test_index in sss.split(idx[subjects_int.get_level_values('subject_id')], Y_int[t]):\n",
    "        # Internal: 10-fold cross validation for training split\n",
    "        best_F1, best_rmse, best_auc, best_auprc = -np.Inf, -np.Inf, -np.Inf, -np.Inf\n",
    "        best_hyperparams = None\n",
    "        early_stop_frac = hyperparams_fixed['early_stop_frac']\n",
    "        best_preds = []\n",
    "        print(\"Evaluating TCN for Outcome: %s, Fold: %d\"%(t,fold))\n",
    "        #train_data = subjects\n",
    "        train_subj = subjects_int[train_index].get_level_values('subject_id')\n",
    "        test_subj = subjects_int[test_index].get_level_values('subject_id')\n",
    "        \n",
    "        [(X_train, X_test), (Ys_train, Ys_test)] = [\n",
    "            [df[df.index.get_level_values('subject_id').isin(s)] for s in (train_subj,  test_subj)] \\\n",
    "            for df in (X_int, Y_int)\n",
    "        ]\n",
    "\n",
    "        for df in X_train, X_test, Ys_train, Ys_test: assert not df.isnull().any().any()\n",
    "            \n",
    "        file1_path = PROCESSED_DATA_DIR + \"SEED\" + str(SEED) + \"_FOLD\" + str(fold) + '_TRAIN_X_train_obs_tmp.h5'\n",
    "        file1_pathlib = pathlib.Path(file1_path)\n",
    "        file2_path = PROCESSED_DATA_DIR + str(SEED) + \"_\" + str(fold) + '_TRAIN_X_train_early_stop_tmp.h5'\n",
    "        file2_pathlib = pathlib.Path(file2_path)\n",
    "        file3_path = PROCESSED_DATA_DIR + str(SEED) + \"_\" + str(fold) + '_TRAIN_X_test_tmp.h5'\n",
    "        file3_pathlib = pathlib.Path(file3_path)\n",
    "        file4_path = PROCESSED_DATA_DIR + str(SEED) + \"_\" + str(fold) + '_TRAIN_Ys_train_obs.h5'\n",
    "        file4_pathlib = pathlib.Path(file4_path)\n",
    "        file5_path = PROCESSED_DATA_DIR + str(SEED) + \"_\" + str(fold)+ '_TRAIN_Ys_train_early_stop.h5'\n",
    "        file5_pathlib = pathlib.Path(file5_path)\n",
    "        file6_path = PROCESSED_DATA_DIR + str(SEED) + \"_\" + str(fold) + '_TRAIN_Ys_test.h5'\n",
    "        file6_pathlib = pathlib.Path(file6_path)\n",
    "\n",
    "        '''\n",
    "        Preprocessing in optimal way takes a lot of time. Therefore, we run the preprocessing with a unique seed and save the dataframe in a more efficient format.\n",
    "        Running the first time will take a while. RUnning again will dive into model training/eval much faster\n",
    "        '''\n",
    "        # if preprocessing for this seed has already been done\n",
    "        if file1_pathlib.exists() and file2_pathlib.exists() and file3_pathlib.exists() and file4_pathlib.exists() and file5_pathlib.exists() and file6_pathlib.exists():\n",
    "            X_train_obs_tmp = pd.read_hdf(file1_path,key='X_train_obs_tmp')\n",
    "            X_train_early_stop_tmp = pd.read_hdf(file2_path,key='X_train_early_stop_tmp')\n",
    "            X_test_tmp = pd.read_hdf(file3_path,key='X_test_tmp')\n",
    "            Ys_train_obs = pd.read_hdf(file4_path, key='Ys_train_obs')\n",
    "            Ys_train_early_stop = pd.read_hdf(file5_path, key='Ys_train_early_stop')\n",
    "            Ys_test = pd.read_hdf(file6_path, key='Ys_test')\n",
    "        else:\n",
    "            set_primary_seeds(SEED)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            \n",
    "            print(\"Train subjects length: \", len(train_subjects))\n",
    "            print(\"ES/valid subjects length: \", len(early_stop_subjects))   \n",
    "\n",
    "            # try normalizing the \"time since measured columns\"\n",
    "            idx = pd.IndexSlice \n",
    "            time_since_normalizer = preprocessing.MinMaxScaler()\n",
    "            time_since_normalizer.fit(X_train.loc[:,idx[:,['time_since_measured']]])\n",
    "            X_train.loc[:,idx[:,['time_since_measured']]] = time_since_normalizer.transform(X_train.loc[:,idx[:,['time_since_measured']]])\n",
    "            X_test.loc[:,idx[:,['time_since_measured']]] = time_since_normalizer.transform(X_test.loc[:,idx[:,['time_since_measured']]])\n",
    "\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "            # if we have a tanh activation we want to cast the 0 labels to -1 in the data splits\n",
    "            if t != 'los_icu':\n",
    "                Ys_train_obs = Ys_train_obs.astype(int).replace(to_replace=-1, value=0)\n",
    "                Ys_train_early_stop = Ys_train_early_stop.astype(int).replace(to_replace=-1, value=0)\n",
    "                Ys_test = Ys_test.astype(int).replace(to_replace=-1, value=0)\n",
    "                Ys_test = Ys_test.astype(int).replace(to_replace=-1, value=0)\n",
    "\n",
    "            X_train_obs_tmp = X_train_obs.copy()\n",
    "            X_train_early_stop_tmp = X_train_early_stop.copy()\n",
    "            X_test_tmp = X_test.copy()\n",
    "\n",
    "            print(\"Stack data - start...\")\n",
    "            manager = multiprocessing.Manager()\n",
    "            shared_dict = manager.dict()\n",
    "\n",
    "            dataframes = [X_train_obs_tmp, X_train_early_stop_tmp, X_test_tmp]\n",
    "            for i,df in enumerate(dataframes):\n",
    "                p = multiprocessing.Process(target=ps_spawn, args=(i,shared_dict,df))\n",
    "                p.start()\n",
    "                p.join()\n",
    "\n",
    "            X_train_obs_tmp = shared_dict[0]\n",
    "            X_train_early_stop_tmp = shared_dict[1]\n",
    "            X_test_tmp = shared_dict[2]\n",
    "\n",
    "            X_train_obs_tmp.to_hdf(file1_path, key='X_train_obs_tmp', mode='w')\n",
    "            X_train_early_stop_tmp.to_hdf(file2_path, key='X_train_early_stop_tmp', mode='w')\n",
    "            X_test_tmp.to_hdf(file3_path, key='X_test_tmp', mode='w')\n",
    "            Ys_train_obs.to_hdf(file4_path, key='Ys_train_obs', mode='w')\n",
    "            Ys_train_early_stop.to_hdf(file5_path, key='Ys_train_early_stop', mode='w')\n",
    "            Ys_test.to_hdf(file6_path, key='Ys_test', mode='w')\n",
    "            print(\"Stack data - complete...\")\n",
    "\n",
    "        if t in ['mort_icu', 'los_3', 'mort_hosp', 'los_7']:\n",
    "            output_last = True\n",
    "        elif t=='los_icu':\n",
    "            output_last = False\n",
    "        else:\n",
    "            print(\"invalid label for 'output_last' check\")\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            model_name = \"Fold%d_ParamSet%d\"%(fold,i)\n",
    "            print(\"Beginning Evaluation for: %s\"%(model_name))\n",
    "            print(\"Hyperparams Set: %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))            \n",
    "            model_hyperparams = copy.copy(hyperparams_fixed)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items()}\n",
    "            )\n",
    "            batch_size = model_hyperparams['batch_size']\n",
    "            \n",
    "            # MULTIVARIATE DL\n",
    "            train_dataloader      = prepare_2d_dataloader(X_train_obs_tmp, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_2d_dataloader(X_train_early_stop_tmp, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "            test_dataloader        = prepare_2d_dataloader(X_test_tmp, Ys_test[t], batch_size=batch_size)\n",
    "            print(\"init dataloaders complete\")\n",
    "            \n",
    "            # Create the model based on the parameters defined above\n",
    "            channel_sizes = [model_hyperparams['nhid']]*model_hyperparams['levels']\n",
    "            model = TCN(model_hyperparams['input_channels'], model_hyperparams['n_classes'], channel_sizes,\\\n",
    "                        kernel_size=model_hyperparams['kernel_size'], dropout=model_hyperparams['dropout'])\n",
    "\n",
    "            set_primary_seeds(SEED)\n",
    "            best_model, _ = TCN_Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader, output_last=output_last,\n",
    "                **{k: v for k, v in model_hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'lr_deg', 'batch_size', 'clip', 'loss_function', 'clamp', 'activity_reg', 'l2_penalty_bool', 'l2_penalty_val', 'amsgrad'\n",
    "                )}\n",
    "            )\n",
    "            \n",
    "            if test_dataloader is not None:\n",
    "                set_primary_seeds(SEED)\n",
    "                probabilities_test, labels_test = predict_proba(best_model,test_dataloader,loss_function=model_hyperparams['loss_function'], output_last=output_last)\n",
    "                #probabilities_dev, labels_dev = np.array(probabilities_dev).flatten(order='C'), np.array(labels_dev).flatten(order='C')\n",
    "                #probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "                y_score           = np.concatenate(probabilities_test)\n",
    "                y_true        = np.concatenate(labels_test)\n",
    "                subject_idx = list(range(0,len(y_score)))\n",
    "                \n",
    "                print(\"Internal validation testing for our best model: %s, on target %s\" % (model_name, t))\n",
    "                if output_last:\n",
    "                    #y_score = probabilities_dev\n",
    "                    if model_hyperparams['loss_function'] == 1:\n",
    "                        rounds = [0, 1]\n",
    "                    elif model_hyperparams['loss_function'] == 2:\n",
    "                        rounds = [-1,1]\n",
    "#                     x = np.subtract.outer(y_score, rounds)\n",
    "#                     cols = np.argmin(abs(x), axis=1).round()\n",
    "#                     y_pred = [rounds[i] for i in cols]\n",
    "                    rounds = [0,1]\n",
    "                    x = np.subtract.outer(y_score, rounds)\n",
    "                    cols = np.argmin(abs(x), axis=1).round()\n",
    "                    y_pred = [rounds[i] for i in cols]\n",
    "\n",
    "                    for elem in y_true:\n",
    "                        if elem not in rounds:\n",
    "                            print(\"numerical error y_trues\")\n",
    "                    for elem in y_pred:\n",
    "                        if elem not in rounds:\n",
    "                            print(\"numerical error pred\")\n",
    "                    auc = roc_auc_score(y_true, y_score)\n",
    "                    auprc = average_precision_score(y_true, y_score)\n",
    "                    acc   = accuracy_score(y_true, y_pred)\n",
    "                    prec = precision_score(y_true, y_pred)\n",
    "                    rec = recall_score(y_true, y_pred)\n",
    "                    F1    = f1_score(y_true, y_pred)\n",
    "                    print(\"auc->%f, auprc->%f, acc->%f, prec->%f, rec->%f, F1->%f\" % (auc, auprc, acc, prec, rec, F1))\n",
    "                    if auc > best_auc:\n",
    "                        best_auc, best_hyperparams = auc, hyperparams\n",
    "                        print(\"New Best AUC within Fold (%d): %.2f @ hyperparams = %s\" % (fold, 100*best_auc, repr((best_hyperparams))))\n",
    "                        # save our best model just in case we want it later\n",
    "#                         torch.save(best_model.module.state_dict(), 'results/best_model.pt')\n",
    "                        best_preds = y_score\n",
    "                else:\n",
    "                    mse = mean_squared_error(y_score, labels_test)\n",
    "                    rmse = sqrt(mse)\n",
    "                    mae = mean_absolute_error(y_score, labels_test)\n",
    "                    pred_mean = np.mean(labels_test)\n",
    "                    pred_std = np.std(labels_test)\n",
    "                    label_mean = np.mean(y_score)\n",
    "                    label_std = np.std(y_score)\n",
    "                    print(\"mse->%f, rmse->%f, MAE->%f, (pred mean, pred_std)->(%f,%f), (label_mean, label_std)->(%f,%f)\" % (mse, rmse, mae, pred_mean, pred_std, label_mean, label_std))\n",
    "#                     results[model_name][t][n] = None, model_hyperparams, mse, rmse\n",
    "                    if rmse > best_rmse:\n",
    "                        best_rmse, best_hyperparams = rmse, hyperparams\n",
    "                        print(\"New Best RMSE within Fold (%d): %.2f @ hyperparams = %s\" % (fold, best_rmse, repr((best_hyperparams))))\n",
    "                        # save our best model just in case we want it later\n",
    "#                         torch.save(best_model.module.state_dict(), 'results/best_model.pt')\n",
    "                        best_preds = y_score\n",
    "        if output_last:\n",
    "            if best_auc > best_fold_auc:\n",
    "                best_fold_auc, best_fold_hyperparams = best_auc, best_hyperparams\n",
    "                best_fold_model_name = model_name\n",
    "                print(\"New Best AUC across all folds: %.2f @ hyperparams = %s\" % (100*best_fold_auc, repr((best_fold_hyperparams))))\n",
    "                # save our best model just in case we want it later\n",
    "#                 torch.save(best_model.module.state_dict(), 'results/best_overall_model.pt')\n",
    "                best_fold = fold\n",
    "        else:\n",
    "            if best_rmse > best_fold_rmse:\n",
    "                best_fold_rmse, best_fold_hyperparams = best_rmse, best_hyperparams\n",
    "                best_fold_model_name = model_name\n",
    "                print(\"New Best RMSE across all folds: %.2f @ hyperparams = %s\" % (best_fold_rmse, repr((best_fold_hyperparams))))\n",
    "                # save our best model just in case we want it later\n",
    "    #                 torch.save(best_model.module.state_dict(), 'results/best_overall_model.pt')\n",
    "                best_fold = fold\n",
    "        assert len(y_true) == len(best_preds), \"Labels (%d) and preds lengths (%d) dont match\"%(len(y_true),len(best_preds))\n",
    "        print(\"Appending best predictions from Fold %d. Length: %d\"%(fold, len(best_preds)))\n",
    "        preds_int.append_logger(subject_idx, y_true, best_preds, label=t, fold=fold)\n",
    "        fold+=1\n",
    "        print()\n",
    "    # External: train/validation and testing on all data\n",
    "    model_hyperparams = copy.copy(hyperparams_fixed)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_fold_hyperparams.items()}\n",
    "    )\n",
    "    early_stop_frac = model_hyperparams['early_stop_frac']\n",
    "    batch_size = model_hyperparams['batch_size']\n",
    "    \n",
    "    print(\"Begin external testing\")\n",
    "    print(\"X internal shape: (%d,%d)\"%(X_int.shape[0]/24,X_int.shape[1]))\n",
    "    print(\"X external shape: (%d,%d)\"%(X_ext.shape[0]/24,X_ext.shape[1]))\n",
    "    print(\"Y internal shape: (%d,%d)\"%(Y_int.shape[0],Y_int.shape[1]))\n",
    "    print(\"Y external shape: (%d,%d)\"%(Y_ext.shape[0],Y_ext.shape[1]))\n",
    "    \n",
    "    file1_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_X_train_obs_tmp.h5'\n",
    "    file1_pathlib = pathlib.Path(file1_path)\n",
    "    file2_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_X_train_early_stop_tmp.h5'\n",
    "    file2_pathlib = pathlib.Path(file2_path)\n",
    "    file3_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_X_test_tmp.h5'\n",
    "    file3_pathlib = pathlib.Path(file3_path)\n",
    "    file4_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_Ys_train_obs.h5'\n",
    "    file4_pathlib = pathlib.Path(file4_path)\n",
    "    file5_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_Ys_train_early_stop.h5'\n",
    "    file5_pathlib = pathlib.Path(file5_path)\n",
    "    file6_path = PROCESSED_DATA_DIR + str(SEED) + '_TEST_Ys_test.h5'\n",
    "    file6_pathlib = pathlib.Path(file6_path)\n",
    "\n",
    "    # preprocessing for this seed has already been done\n",
    "    if file1_pathlib.exists() and file2_pathlib.exists() and file3_pathlib.exists() and file4_pathlib.exists() and file5_pathlib.exists() and file6_pathlib.exists():\n",
    "        X_train_obs_tmp = pd.read_hdf(file1_path,key='X_train_obs_tmp')\n",
    "        X_train_early_stop_tmp = pd.read_hdf(file2_path,key='X_train_early_stop_tmp')\n",
    "        X_test_tmp = pd.read_hdf(file3_path,key='X_test_tmp')\n",
    "        Ys_train_obs = pd.read_hdf(file4_path, key='Ys_train_obs')\n",
    "        Ys_train_early_stop = pd.read_hdf(file5_path, key='Ys_train_early_stop')\n",
    "        Ys_test = pd.read_hdf(file6_path, key='Ys_test')\n",
    "    else:\n",
    "        set_primary_seeds(SEED)\n",
    "        all_train_subjects = list(\n",
    "            np.random.permutation(Y_int.index.get_level_values('subject_id').values)\n",
    "        )\n",
    "        N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "        train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "        early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "\n",
    "        # try normalizing the \"time since measured columns\"\n",
    "        idx = pd.IndexSlice \n",
    "        time_since_normalizer = preprocessing.MinMaxScaler()\n",
    "        time_since_normalizer.fit(X_train.loc[:,idx[:,['time_since_measured']]])\n",
    "        X_int.loc[:,idx[:,['time_since_measured']]] = time_since_normalizer.transform(X_int.loc[:,idx[:,['time_since_measured']]])\n",
    "        X_ext.loc[:,idx[:,['time_since_measured']]] = time_since_normalizer.transform(X_ext.loc[:,idx[:,['time_since_measured']]])\n",
    "\n",
    "        X_train_obs         = X_int[X_int.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        Ys_train_obs        = Y_int[Y_int.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "        X_train_early_stop  = X_int[X_int.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        Ys_train_early_stop = Y_int[Y_int.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "        X_test_tmp = X_ext\n",
    "        Ys_test = Y_ext\n",
    "\n",
    "        # if we have a tanh activation we want to cast the 0 labels to -1 in the data splits\n",
    "        if t != 'los_icu':\n",
    "            Ys_train_obs = Ys_train_obs.astype(int).replace(to_replace=-1, value=0)\n",
    "            Ys_train_early_stop = Ys_train_early_stop.astype(int).replace(to_replace=-1, value=0)\n",
    "            Ys_test = Ys_test.astype(int).replace(to_replace=-1, value=0)\n",
    "\n",
    "        X_train_obs_tmp = X_train_obs.copy()\n",
    "        X_train_early_stop_tmp = X_train_early_stop.copy()\n",
    "        X_test_tmp = X_ext.copy()\n",
    "\n",
    "        print(\"Stack data - start...\")\n",
    "        manager = multiprocessing.Manager()\n",
    "        shared_dict = manager.dict()\n",
    "\n",
    "        dataframes = [X_train_obs_tmp, X_train_early_stop_tmp, X_test_tmp]\n",
    "        for i,df in enumerate(dataframes):\n",
    "            p = multiprocessing.Process(target=ps_spawn, args=(i,shared_dict,df))\n",
    "            p.start()\n",
    "            p.join()\n",
    "\n",
    "        X_train_obs_tmp = shared_dict[0]\n",
    "        X_train_early_stop_tmp = shared_dict[1]\n",
    "        X_test_tmp = shared_dict[2]\n",
    "\n",
    "        X_train_obs_tmp.to_hdf(file1_path, key='X_train_obs_tmp', mode='w')\n",
    "        X_train_early_stop_tmp.to_hdf(file2_path, key='X_train_early_stop_tmp', mode='w')\n",
    "        X_test_tmp.to_hdf(file3_path, key='X_test_tmp', mode='w')\n",
    "        Ys_train_obs.to_hdf(file4_path, key='Ys_train_obs', mode='w')\n",
    "        Ys_train_early_stop.to_hdf(file5_path, key='Ys_train_early_stop', mode='w')\n",
    "        Ys_test.to_hdf(file6_path, key='Ys_test', mode='w')\n",
    "        print(\"Stack data - complete...\")\n",
    "    if t in ['mort_icu', 'los_3', 'mort_hosp', 'los_7']:\n",
    "        output_last = True\n",
    "    elif t=='los_icu':\n",
    "        output_last = False\n",
    "    else:\n",
    "        print(\"invalid label for 'output_last' check\")\n",
    "\n",
    "    # MULTIVARIATE DL\n",
    "    print(\"External Dims: \")\n",
    "    print(\"X internal train: \", X_train_obs_tmp.shape)\n",
    "    print(\"Y internal train: \", Ys_train_obs[t].shape)\n",
    "    print(\"X internal val: \", X_train_early_stop_tmp.shape)\n",
    "    print(\"Y internal val: \", Ys_train_early_stop[t].shape)\n",
    "    print(\"X external test: \", X_test_tmp.shape)\n",
    "    print(\"Y external test: \", Ys_test[t].shape)\n",
    "    train_dataloader      = prepare_2d_dataloader(X_train_obs_tmp, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_2d_dataloader(X_train_early_stop_tmp, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader        = prepare_2d_dataloader(X_test_tmp, Ys_test[t], batch_size=1)\n",
    "    #test_dataloader       = prepare_2d_dataloader(X_test_tmp, Ys_test[t], batch_size=batch_size)\n",
    "    \n",
    "    print(\"init dataloaders complete\")\n",
    "    # Create the model based on the parameters defined above\n",
    "    channel_sizes = [model_hyperparams['nhid']]*model_hyperparams['levels']\n",
    "    model = TCN(model_hyperparams['input_channels'], model_hyperparams['n_classes'], channel_sizes,\\\n",
    "                kernel_size=model_hyperparams['kernel_size'], dropout=model_hyperparams['dropout'])\n",
    "\n",
    "\n",
    "    set_primary_seeds(SEED)\n",
    "    best_model, _ = TCN_Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader, output_last=output_last,\n",
    "        **{k: v for k, v in model_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'lr_deg', 'batch_size', 'clip', 'loss_function', 'clamp', 'activity_reg', 'l2_penalty_bool', 'l2_penalty_val', 'amsgrad'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        set_primary_seeds(SEED)\n",
    "        probabilities_test, labels_test = predict_proba(best_model,test_dataloader,loss_function=model_hyperparams['loss_function'], output_last=output_last)\n",
    "        y_score           = np.concatenate(probabilities_test)\n",
    "        y_true        = np.concatenate(labels_test)\n",
    "        subject_idx = list(range(0,len(y_score)))\n",
    "        \n",
    "        #y_score = probabilities_dev\n",
    "        print(\"External validation testing for our best model: %s, on target %s\" % (best_fold_model_name, t))\n",
    "        print(\"(hyperparams = %s)\" % (repr((model_hyperparams))))\n",
    "        if output_last:\n",
    "            if model_hyperparams['loss_function'] == 1:\n",
    "                rounds = [0, 1]\n",
    "            elif model_hyperparams['loss_function'] == 2:\n",
    "                rounds = [-1,1]\n",
    "            x = np.subtract.outer(y_score, rounds)\n",
    "            cols = np.argmin(abs(x), axis=1).round()\n",
    "            y_pred = [rounds[i] for i in cols]\n",
    "\n",
    "            for elem in y_true:\n",
    "                if elem not in rounds:\n",
    "                    print(\"numerical error y_trues\")\n",
    "            for elem in y_pred:\n",
    "                if elem not in rounds:\n",
    "                    print(\"numerical error pred\")\n",
    "\n",
    "            auc   = roc_auc_score(y_true, y_score)\n",
    "            auprc = average_precision_score(y_true, y_score)\n",
    "            acc   = accuracy_score(y_true, y_pred)\n",
    "            prec  = precision_score(y_true, y_pred)\n",
    "            rec   = recall_score(y_true, y_pred)\n",
    "            F1    = f1_score(y_true, y_pred)\n",
    "            print(\"auc->%f, auprc->%f, acc->%f, prec->%f, rec->%f, F1->%f\" % (auc, auprc, acc, prec, rec, F1))\n",
    "#             results[model_name][t][n] = None, model_hyperparams, auc, auprc, acc, F1, best_auc\n",
    "        else:\n",
    "            mse = mean_squared_error(y_score, labels_test)\n",
    "            rmse = sqrt(mse)\n",
    "            mae = mean_absolute_error(y_score, labels_test)\n",
    "            pred_mean = np.mean(labels_test)\n",
    "            pred_std = np.std(labels_test)\n",
    "            label_mean = np.mean(y_score)\n",
    "            label_std = np.std(y_score)\n",
    "\n",
    "#             print(\"RMSE(l2)       ---        MSE        ---        MAE(l1)        ---        (pred_mean, pred_std)        ---        (label_mean, label_std)\")\n",
    "            print(\"mse->%f, rmse->%f, MAE->%f, (pred mean, pred_std)->(%f,%f), (label_mean, label_std)->(%f,%f)\" % (mse, rmse, mae, pred_mean, pred_std, label_mean, label_std))\n",
    "#             results[model_name][t][n] = None, model_hyperparams, mse, rmse\n",
    "        assert len(y_true) == len(y_score), \"Labels (%d) and preds lengths (%d) dont match\"%(len(y_true),len(best_preds))\n",
    "        print(\"Appending external test predictions, length: %d\"%(len(best_preds)))\n",
    "        preds_ext.append_logger(subject_idx, y_true, y_score, label=t, fold=-1)\n",
    "#     with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)\n",
    "    print(\"\\n\")\n",
    "preds_int.df = preds_df_to_int(preds_int.df)\n",
    "preds_int.df.to_csv(RESULTS_DIR+\"TCN_internal_test_preds.csv\")\n",
    "preds_ext.df = preds_df_to_int(preds_ext.df)\n",
    "preds_ext.df.to_csv(RESULTS_DIR+\"TCN_external_test_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write cell output (text) from above into a dedicate logfile\n",
    "'''\n",
    "write_file = RESULTS_DIR + 'TCN_main_output.txt'\n",
    "with open(write_file, 'w') as f:\n",
    "    f.write(cap.stdout)\n",
    "del(cap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_dev)",
   "language": "python",
   "name": "pytorch_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
